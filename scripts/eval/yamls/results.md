Printing gauntlet results for all models
| model_name               |   average |   world_knowledge |   commonsense_reasoning |   language_understanding |   symbolic_problem_solving |   reading_comprehension |
|:-------------------------|----------:|------------------:|------------------------:|-------------------------:|---------------------------:|------------------------:|
| llama-30b    |  0.508013 |          0.570561 |                0.521302 |                 0.549439 |                   0.321474 |                0.577292 |
| falcon-40b   |  0.470544 |          0.462484 |                0.550473 |                 0.535191 |                   0.268338 |                0.536235 |
| huggyllama/llama-13b                |  0.428223 |          0.511058 |                0.464285 |                 0.482423 |                  0.23844   |                0.444907 |
| huggyllama/llama-7b |  0.351241 |          0.354118 |                0.396072 |                 0.428827 |                   0.182015 |                0.395171 |
| togethercomputer/RedPajama-INCITE-7B-Instruct |  0.354936 |          0.368793 |                0.367142 |                 0.395898 |                   0.210048 |                0.432801 |
| mosaicml/mpt-7b-instruct |  0.338077 |          0.338253 |                0.416911 |                 0.371509 |                   0.17265  |                0.391062 |
| mosaicml/mpt-7b-chat |  0.335942 |          0.326105 |                0.397677 |                 0.383453 |                    0.17976 |                0.392714 |
| mosaicml/mpt-7b          |  0.310326 |          0.310191 |                0.384509 |                 0.380392 |                   0.162957 |                0.31358  |
| tiiuae/falcon-7b         |  0.309822 |          0.272142 |                0.419968 |                 0.369998 |                   0.158363 |                0.328637 |
| togethercomputer/RedPajama-INCITE-7B-Base     |  0.29738  |          0.312032 |                0.363261 |                 0.3733   |                   0.126577 |                0.311731 |
| tiiuae/falcon-7b-instruct                     |  0.28197  |          0.260288 |                0.370308 |                 0.332523 |                   0.107958 |                0.338774 |
| EleutherAI/pythia-12b  |  0.274429 |          0.252255 |                0.344973 |                 0.33249  |                   0.136118 |                0.306308 |
| EleutherAI/gpt-j-6b                 |  0.268168 |          0.260849 |                0.330648 |                 0.311813 |                  0.120669  |                0.31686  |
| facebook/opt-6.7b                   |  0.24994  |          0.236678 |                0.326348 |                 0.322621 |                  0.0930295 |                0.271022 |
| EleutherAI/pythia-6.9b              |  0.248811 |          0.218628 |                0.308817 |                 0.304028 |                  0.120792  |                0.291793 |
| stabilityai/stablelm-tuned-alpha-7b |  0.163522 |          0.129503 |                0.198957 |                 0.20249  |                  0.093985  |                0.192676 |
Printing complete results for all models
| Category                 | Benchmark                        | Subtask                             |   Accuracy |   Number few shot | Model                    |
|:-------------------------|:---------------------------------|:------------------------------------|-----------:|------------------:|:-------------------------|
| world_knowledge          | jeopardy                         | Average                             | 0.330625   |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | american_history                    | 0.428571   |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | literature                          | 0.371429   |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | science                             | 0.203782   |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | word_origins                        | 0.150685   |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | world_history                       | 0.49866    |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          | triviaqa                         |                                     | 0.330593   |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          | bigbench_qa_wikidata             |                                     | 0.38128    |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          | arc_easy                         |                                     | 0.699916   |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          | arc_challenge                    |                                     | 0.43942    |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          | mmlu                             | Average                             | 0.32856    |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | abstract_algebra                    | 0.22       |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | anatomy                             | 0.422222   |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | astronomy                           | 0.282895   |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | business_ethics                     | 0.34       |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | clinical_knowledge                  | 0.369811   |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | college_biology                     | 0.333333   |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | college_chemistry                   | 0.26       |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | college_computer_science            | 0.26       |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | college_mathematics                 | 0.29       |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | college_medicine                    | 0.312139   |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | college_physics                     | 0.254902   |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | computer_security                   | 0.42       |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | conceptual_physics                  | 0.285106   |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | econometrics                        | 0.289474   |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | electrical_engineering              | 0.372414   |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | elementary_mathematics              | 0.256614   |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | formal_logic                        | 0.261905   |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | global_facts                        | 0.32       |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | high_school_biology                 | 0.351613   |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | high_school_chemistry               | 0.295566   |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | high_school_computer_science        | 0.32       |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | high_school_european_history        | 0.315152   |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | high_school_geography               | 0.434343   |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | high_school_government_and_politics | 0.455959   |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | high_school_macroeconomics          | 0.35641    |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | high_school_mathematics             | 0.218519   |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | high_school_microeconomics          | 0.37395    |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | high_school_physics                 | 0.311258   |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | high_school_psychology              | 0.33578    |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | high_school_statistics              | 0.273148   |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | high_school_us_history              | 0.29902    |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | high_school_world_history           | 0.2827     |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | human_aging                         | 0.255605   |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | human_sexuality                     | 0.358779   |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | international_law                   | 0.421488   |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | jurisprudence                       | 0.314815   |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | logical_fallacies                   | 0.288344   |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | machine_learning                    | 0.285714   |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | management                          | 0.320388   |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | marketing                           | 0.418803   |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | medical_genetics                    | 0.32       |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | miscellaneous                       | 0.413793   |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | moral_disputes                      | 0.32659    |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | moral_scenarios                     | 0.242458   |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | nutrition                           | 0.359477   |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | philosophy                          | 0.356913   |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | prehistory                          | 0.416667   |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | professional_accounting             | 0.308511   |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | professional_law                    | 0.29661    |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | professional_medicine               | 0.253676   |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | professional_psychology             | 0.305556   |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | public_relations                    | 0.409091   |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | security_studies                    | 0.379592   |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | sociology                           | 0.348259   |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | us_foreign_policy                   | 0.38       |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | virology                            | 0.421687   |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | world_religions                     | 0.350877   |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          | bigbench_misconceptions          |                                     | 0.584475   |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          | bigbench_movie_recommendation    |                                     | 0.26       |                 0 | mosaicml/mpt-7b-instruct |
| commonsense_reasoning    | copa                             |                                     | 0.83       |                 0 | mosaicml/mpt-7b-instruct |
| commonsense_reasoning    | piqa                             |                                     | 0.798694   |                 0 | mosaicml/mpt-7b-instruct |
| commonsense_reasoning    | openbook_qa                      |                                     | 0.436      |                 0 | mosaicml/mpt-7b-instruct |
| commonsense_reasoning    | bigbench_novel_concepts          |                                     | 0.53125    |                 0 | mosaicml/mpt-7b-instruct |
| commonsense_reasoning    | bigbench_strange_stories         |                                     | 0.614943   |                 0 | mosaicml/mpt-7b-instruct |
| commonsense_reasoning    | bigbench_strategy_qa             |                                     | 0.587593   |                 0 | mosaicml/mpt-7b-instruct |
| language_understanding   | lambada_openai                   |                                     | 0.69086    |                 0 | mosaicml/mpt-7b-instruct |
| language_understanding   | hellaswag                        |                                     | 0.769966   |                 0 | mosaicml/mpt-7b-instruct |
| language_understanding   | winograd                         |                                     | 0.846154   |                 0 | mosaicml/mpt-7b-instruct |
| language_understanding   | winogrande                       |                                     | 0.67719    |                 0 | mosaicml/mpt-7b-instruct |
| language_understanding   | bigbench_conlang_translation     |                                     | 0.0670732  |                 0 | mosaicml/mpt-7b-instruct |
| language_understanding   | bigbench_language_identification |                                     | 0.3009     |                 0 | mosaicml/mpt-7b-instruct |
| language_understanding   | bigbench_conceptual_combinations |                                     | 0.427184   |                 0 | mosaicml/mpt-7b-instruct |
| symbolic_problem_solving | bigbench_elementary_math_qa      |                                     | 0.263889   |                 0 | mosaicml/mpt-7b-instruct |
| symbolic_problem_solving | bigbench_dyck_languages          |                                     | 0.067      |                 0 | mosaicml/mpt-7b-instruct |
| symbolic_problem_solving | bigbench_cs_algorithms           |                                     | 0.0598485  |                 0 | mosaicml/mpt-7b-instruct |
| symbolic_problem_solving | bigbench_logical_deduction       |                                     | 0.324      |                 0 | mosaicml/mpt-7b-instruct |
| symbolic_problem_solving | bigbench_operators               |                                     | 0.309524   |                 0 | mosaicml/mpt-7b-instruct |
| symbolic_problem_solving | bigbench_repeat_copy_logic       |                                     | 0.03125    |                 0 | mosaicml/mpt-7b-instruct |
| symbolic_problem_solving | simple_arithmetic_nospaces       |                                     | 0          |                 0 | mosaicml/mpt-7b-instruct |
| symbolic_problem_solving | simple_arithmetic_withspaces     |                                     | 0.021      |                 0 | mosaicml/mpt-7b-instruct |
| symbolic_problem_solving | math_qa                          |                                     | 0.254777   |                 0 | mosaicml/mpt-7b-instruct |
| symbolic_problem_solving | logi_qa                          |                                     | 0.24424    |                 0 | mosaicml/mpt-7b-instruct |
| reading_comprehension    | squad                            |                                     | 0.201514   |                 0 | mosaicml/mpt-7b-instruct |
| reading_comprehension    | coqa                             |                                     | 0.198296   |                 0 | mosaicml/mpt-7b-instruct |
| reading_comprehension    | bigbench_understanding_fables    |                                     | 0.31746    |                 0 | mosaicml/mpt-7b-instruct |
| reading_comprehension    | boolq                            |                                     | 0.768196   |                 0 | mosaicml/mpt-7b-instruct |
| world_knowledge          | jeopardy                         | Average                             | 0.457462   |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | american_history                    | 0.51816    |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | literature                          | 0.55102    |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | science                             | 0.357143   |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | word_origins                        | 0.268493   |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | world_history                       | 0.592493   |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          | bigbench_qa_wikidata             |                                     | 0.694503   |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          | arc_easy                         |                                     | 0.752525   |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          | arc_challenge                    |                                     | 0.477816   |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          | mmlu                             | Average                             | 0.301998   |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | abstract_algebra                    | 0.32       |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | anatomy                             | 0.251852   |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | astronomy                           | 0.322368   |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | business_ethics                     | 0.23       |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | clinical_knowledge                  | 0.316981   |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | college_biology                     | 0.277778   |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | college_chemistry                   | 0.3        |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | college_computer_science            | 0.29       |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | college_mathematics                 | 0.29       |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | college_medicine                    | 0.283237   |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | college_physics                     | 0.264706   |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | computer_security                   | 0.32       |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | conceptual_physics                  | 0.340426   |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | econometrics                        | 0.184211   |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | electrical_engineering              | 0.289655   |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | elementary_mathematics              | 0.256614   |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | formal_logic                        | 0.261905   |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | global_facts                        | 0.3        |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | high_school_biology                 | 0.325806   |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | high_school_chemistry               | 0.26601    |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | high_school_computer_science        | 0.31       |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | high_school_european_history        | 0.266667   |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | high_school_geography               | 0.39899    |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | high_school_government_and_politics | 0.362694   |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | high_school_macroeconomics          | 0.320513   |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | high_school_mathematics             | 0.251852   |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | high_school_microeconomics          | 0.327731   |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | high_school_physics                 | 0.291391   |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | high_school_psychology              | 0.311927   |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | high_school_statistics              | 0.347222   |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | high_school_us_history              | 0.269608   |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | high_school_world_history           | 0.257384   |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | human_aging                         | 0.363229   |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | human_sexuality                     | 0.358779   |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | international_law                   | 0.289256   |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | jurisprudence                       | 0.314815   |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | logical_fallacies                   | 0.226994   |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | machine_learning                    | 0.285714   |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | management                          | 0.281553   |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | marketing                           | 0.294872   |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | medical_genetics                    | 0.33       |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | miscellaneous                       | 0.381865   |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | moral_disputes                      | 0.346821   |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | moral_scenarios                     | 0.240223   |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | nutrition                           | 0.330065   |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | philosophy                          | 0.315113   |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | prehistory                          | 0.333333   |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | professional_accounting             | 0.244681   |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | professional_law                    | 0.264016   |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | professional_medicine               | 0.220588   |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | professional_psychology             | 0.271242   |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | public_relations                    | 0.454545   |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | security_studies                    | 0.355102   |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | sociology                           | 0.308458   |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | us_foreign_policy                   | 0.34       |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | virology                            | 0.403614   |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          |                                  | world_religions                     | 0.251462   |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          | bigbench_misconceptions          |                                     | 0.598173   |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          | bigbench_movie_recommendation    |                                     | 0.238      |                10 | mosaicml/mpt-7b-instruct |
| commonsense_reasoning    | piqa                             |                                     | 0.811752   |                10 | mosaicml/mpt-7b-instruct |
| commonsense_reasoning    | bigbench_novel_concepts          |                                     | 0.53125    |                10 | mosaicml/mpt-7b-instruct |
| commonsense_reasoning    | bigbench_strange_stories         |                                     | 0.701149   |                10 | mosaicml/mpt-7b-instruct |
| commonsense_reasoning    | bigbench_strategy_qa             |                                     | 0.59633    |                10 | mosaicml/mpt-7b-instruct |
| language_understanding   | hellaswag                        |                                     | 0.769767   |                10 | mosaicml/mpt-7b-instruct |
| language_understanding   | bigbench_conlang_translation     |                                     | 0.0426829  |                10 | mosaicml/mpt-7b-instruct |
| language_understanding   | bigbench_language_identification |                                     | 0.2568     |                10 | mosaicml/mpt-7b-instruct |
| language_understanding   | bigbench_conceptual_combinations |                                     | 0.320388   |                10 | mosaicml/mpt-7b-instruct |
| symbolic_problem_solving | bigbench_elementary_math_qa      |                                     | 0.270466   |                10 | mosaicml/mpt-7b-instruct |
| symbolic_problem_solving | bigbench_dyck_languages          |                                     | 0.314      |                10 | mosaicml/mpt-7b-instruct |
| symbolic_problem_solving | bigbench_cs_algorithms           |                                     | 0.496212   |                10 | mosaicml/mpt-7b-instruct |
| symbolic_problem_solving | bigbench_logical_deduction       |                                     | 0.262667   |                10 | mosaicml/mpt-7b-instruct |
| symbolic_problem_solving | bigbench_operators               |                                     | 0.352381   |                10 | mosaicml/mpt-7b-instruct |
| symbolic_problem_solving | bigbench_repeat_copy_logic       |                                     | 0.3125     |                10 | mosaicml/mpt-7b-instruct |
| symbolic_problem_solving | simple_arithmetic_nospaces       |                                     | 0.078      |                10 | mosaicml/mpt-7b-instruct |
| symbolic_problem_solving | simple_arithmetic_withspaces     |                                     | 0.086      |                10 | mosaicml/mpt-7b-instruct |
| symbolic_problem_solving | math_qa                          |                                     | 0.257459   |                10 | mosaicml/mpt-7b-instruct |
| symbolic_problem_solving | logi_qa                          |                                     | 0.274962   |                10 | mosaicml/mpt-7b-instruct |
| reading_comprehension    | pubmed_qa_labeled                |                                     | 0.59       |                10 | mosaicml/mpt-7b-instruct |
| reading_comprehension    | squad                            |                                     | 0.586944   |                10 | mosaicml/mpt-7b-instruct |
| reading_comprehension    | coqa                             |                                     | 0.287987   |                10 | mosaicml/mpt-7b-instruct |
| reading_comprehension    | bigbench_understanding_fables    |                                     | 0.195767   |                10 | mosaicml/mpt-7b-instruct |
| reading_comprehension    | boolq                            |                                     | 0.781346   |                10 | mosaicml/mpt-7b-instruct |
| world_knowledge          | jeopardy                         | Average                             | 0.151303   |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | american_history                    | 0.232446   |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | literature                          | 0.120408   |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | science                             | 0.0861345  |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | word_origins                        | 0.0547945  |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | world_history                       | 0.262735   |                 0 | tiiuae/falcon-7b         |
| world_knowledge          | triviaqa                         |                                     | 0.00106073 |                 0 | tiiuae/falcon-7b         |
| world_knowledge          | bigbench_qa_wikidata             |                                     | 0.390335   |                 0 | tiiuae/falcon-7b         |
| world_knowledge          | arc_easy                         |                                     | 0.673822   |                 0 | tiiuae/falcon-7b         |
| world_knowledge          | arc_challenge                    |                                     | 0.418089   |                 0 | tiiuae/falcon-7b         |
| world_knowledge          | mmlu                             | Average                             | 0.262612   |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | abstract_algebra                    | 0.3        |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | anatomy                             | 0.251852   |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | astronomy                           | 0.243421   |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | business_ethics                     | 0.22       |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | clinical_knowledge                  | 0.290566   |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | college_biology                     | 0.243056   |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | college_chemistry                   | 0.24       |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | college_computer_science            | 0.21       |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | college_mathematics                 | 0.22       |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | college_medicine                    | 0.306358   |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | college_physics                     | 0.245098   |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | computer_security                   | 0.29       |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | conceptual_physics                  | 0.297872   |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | econometrics                        | 0.280702   |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | electrical_engineering              | 0.255172   |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | elementary_mathematics              | 0.248677   |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | formal_logic                        | 0.206349   |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | global_facts                        | 0.34       |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | high_school_biology                 | 0.229032   |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | high_school_chemistry               | 0.270936   |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | high_school_computer_science        | 0.21       |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | high_school_european_history        | 0.212121   |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | high_school_geography               | 0.212121   |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | high_school_government_and_politics | 0.227979   |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | high_school_macroeconomics          | 0.230769   |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | high_school_mathematics             | 0.262963   |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | high_school_microeconomics          | 0.273109   |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | high_school_physics                 | 0.245033   |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | high_school_psychology              | 0.207339   |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | high_school_statistics              | 0.194444   |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | high_school_us_history              | 0.25       |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | high_school_world_history           | 0.308017   |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | human_aging                         | 0.349776   |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | human_sexuality                     | 0.251908   |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | international_law                   | 0.247934   |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | jurisprudence                       | 0.287037   |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | logical_fallacies                   | 0.226994   |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | machine_learning                    | 0.348214   |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | management                          | 0.281553   |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | marketing                           | 0.286325   |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | medical_genetics                    | 0.24       |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | miscellaneous                       | 0.301405   |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | moral_disputes                      | 0.257225   |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | moral_scenarios                     | 0.237989   |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | nutrition                           | 0.245098   |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | philosophy                          | 0.263666   |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | prehistory                          | 0.290123   |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | professional_accounting             | 0.269504   |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | professional_law                    | 0.242503   |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | professional_medicine               | 0.154412   |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | professional_psychology             | 0.236928   |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | public_relations                    | 0.345455   |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | security_studies                    | 0.285714   |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | sociology                           | 0.338308   |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | us_foreign_policy                   | 0.31       |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | virology                            | 0.355422   |                 0 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | world_religions                     | 0.292398   |                 0 | tiiuae/falcon-7b         |
| world_knowledge          | bigbench_misconceptions          |                                     | 0.525114   |                 0 | tiiuae/falcon-7b         |
| world_knowledge          | bigbench_movie_recommendation    |                                     | 0.242      |                 0 | tiiuae/falcon-7b         |
| commonsense_reasoning    | copa                             |                                     | 0.84       |                 0 | tiiuae/falcon-7b         |
| commonsense_reasoning    | piqa                             |                                     | 0.797606   |                 0 | tiiuae/falcon-7b         |
| commonsense_reasoning    | openbook_qa                      |                                     | 0.426      |                 0 | tiiuae/falcon-7b         |
| commonsense_reasoning    | bigbench_novel_concepts          |                                     | 0.375      |                 0 | tiiuae/falcon-7b         |
| commonsense_reasoning    | bigbench_strange_stories         |                                     | 0.643678   |                 0 | tiiuae/falcon-7b         |
| commonsense_reasoning    | bigbench_strategy_qa             |                                     | 0.56007    |                 0 | tiiuae/falcon-7b         |
| language_understanding   | lambada_openai                   |                                     | 0.746555   |                 0 | tiiuae/falcon-7b         |
| language_understanding   | hellaswag                        |                                     | 0.771759   |                 0 | tiiuae/falcon-7b         |
| language_understanding   | winograd                         |                                     | 0.846154   |                 0 | tiiuae/falcon-7b         |
| language_understanding   | winogrande                       |                                     | 0.674033   |                 0 | tiiuae/falcon-7b         |
| language_understanding   | bigbench_conlang_translation     |                                     | 0.0670732  |                 0 | tiiuae/falcon-7b         |
| language_understanding   | bigbench_language_identification |                                     | 0.2581     |                 0 | tiiuae/falcon-7b         |
| language_understanding   | bigbench_conceptual_combinations |                                     | 0.31068    |                 0 | tiiuae/falcon-7b         |
| symbolic_problem_solving | bigbench_elementary_math_qa      |                                     | 0.267531   |                 0 | tiiuae/falcon-7b         |
| symbolic_problem_solving | bigbench_dyck_languages          |                                     | 0          |                 0 | tiiuae/falcon-7b         |
| symbolic_problem_solving | bigbench_cs_algorithms           |                                     | 0.0356061  |                 0 | tiiuae/falcon-7b         |
| symbolic_problem_solving | bigbench_logical_deduction       |                                     | 0.251333   |                 0 | tiiuae/falcon-7b         |
| symbolic_problem_solving | bigbench_operators               |                                     | 0.209524   |                 0 | tiiuae/falcon-7b         |
| symbolic_problem_solving | bigbench_repeat_copy_logic       |                                     | 0          |                 0 | tiiuae/falcon-7b         |
| symbolic_problem_solving | simple_arithmetic_nospaces       |                                     | 0.014      |                 0 | tiiuae/falcon-7b         |
| symbolic_problem_solving | simple_arithmetic_withspaces     |                                     | 0.095      |                 0 | tiiuae/falcon-7b         |
| symbolic_problem_solving | math_qa                          |                                     | 0.254777   |                 0 | tiiuae/falcon-7b         |
| symbolic_problem_solving | logi_qa                          |                                     | 0.204301   |                 0 | tiiuae/falcon-7b         |
| reading_comprehension    | squad                            |                                     | 0.166225   |                 0 | tiiuae/falcon-7b         |
| reading_comprehension    | coqa                             |                                     | 0.182638   |                 0 | tiiuae/falcon-7b         |
| reading_comprehension    | bigbench_understanding_fables    |                                     | 0.280423   |                 0 | tiiuae/falcon-7b         |
| reading_comprehension    | boolq                            |                                     | 0.725994   |                 0 | tiiuae/falcon-7b         |
| world_knowledge          | jeopardy                         | Average                             | 0.454651   |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | american_history                    | 0.513317   |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | literature                          | 0.591837   |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | science                             | 0.342437   |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | word_origins                        | 0.246575   |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | world_history                       | 0.579088   |                10 | tiiuae/falcon-7b         |
| world_knowledge          | bigbench_qa_wikidata             |                                     | 0.700507   |                10 | tiiuae/falcon-7b         |
| world_knowledge          | arc_easy                         |                                     | 0.75       |                10 | tiiuae/falcon-7b         |
| world_knowledge          | arc_challenge                    |                                     | 0.463311   |                10 | tiiuae/falcon-7b         |
| world_knowledge          | mmlu                             | Average                             | 0.271857   |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | abstract_algebra                    | 0.32       |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | anatomy                             | 0.274074   |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | astronomy                           | 0.236842   |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | business_ethics                     | 0.26       |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | clinical_knowledge                  | 0.298113   |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | college_biology                     | 0.256944   |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | college_chemistry                   | 0.21       |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | college_computer_science            | 0.24       |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | college_mathematics                 | 0.22       |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | college_medicine                    | 0.254335   |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | college_physics                     | 0.27451    |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | computer_security                   | 0.26       |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | conceptual_physics                  | 0.319149   |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | econometrics                        | 0.22807    |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | electrical_engineering              | 0.255172   |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | elementary_mathematics              | 0.243386   |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | formal_logic                        | 0.246032   |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | global_facts                        | 0.3        |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | high_school_biology                 | 0.277419   |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | high_school_chemistry               | 0.251232   |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | high_school_computer_science        | 0.26       |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | high_school_european_history        | 0.266667   |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | high_school_geography               | 0.282828   |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | high_school_government_and_politics | 0.264249   |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | high_school_macroeconomics          | 0.24359    |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | high_school_mathematics             | 0.259259   |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | high_school_microeconomics          | 0.226891   |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | high_school_physics                 | 0.251656   |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | high_school_psychology              | 0.249541   |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | high_school_statistics              | 0.180556   |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | high_school_us_history              | 0.220588   |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | high_school_world_history           | 0.303797   |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | human_aging                         | 0.412556   |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | human_sexuality                     | 0.312977   |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | international_law                   | 0.256198   |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | jurisprudence                       | 0.296296   |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | logical_fallacies                   | 0.263804   |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | machine_learning                    | 0.375      |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | management                          | 0.242718   |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | marketing                           | 0.264957   |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | medical_genetics                    | 0.26       |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | miscellaneous                       | 0.300128   |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | moral_disputes                      | 0.294798   |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | moral_scenarios                     | 0.252514   |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | nutrition                           | 0.30719    |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | philosophy                          | 0.250804   |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | prehistory                          | 0.287037   |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | professional_accounting             | 0.280142   |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | professional_law                    | 0.255541   |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | professional_medicine               | 0.165441   |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | professional_psychology             | 0.254902   |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | public_relations                    | 0.263636   |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | security_studies                    | 0.35102    |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | sociology                           | 0.338308   |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | us_foreign_policy                   | 0.35       |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | virology                            | 0.373494   |                10 | tiiuae/falcon-7b         |
| world_knowledge          |                                  | world_religions                     | 0.251462   |                10 | tiiuae/falcon-7b         |
| world_knowledge          | bigbench_misconceptions          |                                     | 0.52968    |                10 | tiiuae/falcon-7b         |
| world_knowledge          | bigbench_movie_recommendation    |                                     | 0.236      |                10 | tiiuae/falcon-7b         |
| commonsense_reasoning    | piqa                             |                                     | 0.805223   |                10 | tiiuae/falcon-7b         |
| commonsense_reasoning    | bigbench_novel_concepts          |                                     | 0.53125    |                10 | tiiuae/falcon-7b         |
| commonsense_reasoning    | bigbench_strange_stories         |                                     | 0.712644   |                10 | tiiuae/falcon-7b         |
| commonsense_reasoning    | bigbench_strategy_qa             |                                     | 0.597204   |                10 | tiiuae/falcon-7b         |
| language_understanding   | hellaswag                        |                                     | 0.775742   |                10 | tiiuae/falcon-7b         |
| language_understanding   | bigbench_conlang_translation     |                                     | 0.0487805  |                10 | tiiuae/falcon-7b         |
| language_understanding   | bigbench_language_identification |                                     | 0.2544     |                10 | tiiuae/falcon-7b         |
| language_understanding   | bigbench_conceptual_combinations |                                     | 0.271845   |                10 | tiiuae/falcon-7b         |
| symbolic_problem_solving | bigbench_elementary_math_qa      |                                     | 0.271829   |                10 | tiiuae/falcon-7b         |
| symbolic_problem_solving | bigbench_dyck_languages          |                                     | 0.233      |                10 | tiiuae/falcon-7b         |
| symbolic_problem_solving | bigbench_cs_algorithms           |                                     | 0.468939   |                10 | tiiuae/falcon-7b         |
| symbolic_problem_solving | bigbench_logical_deduction       |                                     | 0.256667   |                10 | tiiuae/falcon-7b         |
| symbolic_problem_solving | bigbench_operators               |                                     | 0.295238   |                10 | tiiuae/falcon-7b         |
| symbolic_problem_solving | bigbench_repeat_copy_logic       |                                     | 0.21875    |                10 | tiiuae/falcon-7b         |
| symbolic_problem_solving | simple_arithmetic_nospaces       |                                     | 0.15       |                10 | tiiuae/falcon-7b         |
| symbolic_problem_solving | simple_arithmetic_withspaces     |                                     | 0.178      |                10 | tiiuae/falcon-7b         |
| symbolic_problem_solving | math_qa                          |                                     | 0.25243    |                10 | tiiuae/falcon-7b         |
| symbolic_problem_solving | logi_qa                          |                                     | 0.248848   |                10 | tiiuae/falcon-7b         |
| reading_comprehension    | pubmed_qa_labeled                |                                     | 0.474      |                10 | tiiuae/falcon-7b         |
| reading_comprehension    | squad                            |                                     | 0.490161   |                10 | tiiuae/falcon-7b         |
| reading_comprehension    | coqa                             |                                     | 0.242014   |                10 | tiiuae/falcon-7b         |
| reading_comprehension    | bigbench_understanding_fables    |                                     | 0.232804   |                10 | tiiuae/falcon-7b         |
| reading_comprehension    | boolq                            |                                     | 0.729969   |                10 | tiiuae/falcon-7b         |
| world_knowledge          | jeopardy                         | Average                             | 0.314488   |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | american_history                    | 0.428571   |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | literature                          | 0.314286   |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | science                             | 0.210084   |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | word_origins                        | 0.134247   |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | world_history                       | 0.485255   |                 0 | mosaicml/mpt-7b          |
| world_knowledge          | triviaqa                         |                                     | 0.343145   |                 0 | mosaicml/mpt-7b          |
| world_knowledge          | bigbench_qa_wikidata             |                                     | 0.397077   |                 0 | mosaicml/mpt-7b          |
| world_knowledge          | arc_easy                         |                                     | 0.699074   |                 0 | mosaicml/mpt-7b          |
| world_knowledge          | arc_challenge                    |                                     | 0.409556   |                 0 | mosaicml/mpt-7b          |
| world_knowledge          | mmlu                             | Average                             | 0.26555    |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | abstract_algebra                    | 0.31       |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | anatomy                             | 0.348148   |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | astronomy                           | 0.309211   |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | business_ethics                     | 0.25       |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | clinical_knowledge                  | 0.264151   |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | college_biology                     | 0.284722   |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | college_chemistry                   | 0.18       |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | college_computer_science            | 0.25       |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | college_mathematics                 | 0.25       |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | college_medicine                    | 0.271676   |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | college_physics                     | 0.215686   |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | computer_security                   | 0.28       |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | conceptual_physics                  | 0.2        |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | econometrics                        | 0.245614   |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | electrical_engineering              | 0.296552   |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | elementary_mathematics              | 0.26455    |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | formal_logic                        | 0.222222   |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | global_facts                        | 0.32       |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | high_school_biology                 | 0.264516   |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | high_school_chemistry               | 0.285714   |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | high_school_computer_science        | 0.34       |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | high_school_european_history        | 0.284848   |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | high_school_geography               | 0.267677   |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | high_school_government_and_politics | 0.248705   |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | high_school_macroeconomics          | 0.215385   |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | high_school_mathematics             | 0.27037    |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | high_school_microeconomics          | 0.231092   |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | high_school_physics                 | 0.284768   |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | high_school_psychology              | 0.223853   |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | high_school_statistics              | 0.189815   |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | high_school_us_history              | 0.264706   |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | high_school_world_history           | 0.257384   |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | human_aging                         | 0.179372   |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | human_sexuality                     | 0.244275   |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | international_law                   | 0.371901   |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | jurisprudence                       | 0.25       |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | logical_fallacies                   | 0.343558   |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | machine_learning                    | 0.258929   |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | management                          | 0.213592   |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | marketing                           | 0.294872   |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | medical_genetics                    | 0.24       |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | miscellaneous                       | 0.335888   |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | moral_disputes                      | 0.286127   |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | moral_scenarios                     | 0.246927   |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | nutrition                           | 0.267974   |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | philosophy                          | 0.305466   |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | prehistory                          | 0.308642   |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | professional_accounting             | 0.258865   |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | professional_law                    | 0.282269   |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | professional_medicine               | 0.183824   |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | professional_psychology             | 0.289216   |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | public_relations                    | 0.272727   |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | security_studies                    | 0.253061   |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | sociology                           | 0.263682   |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | us_foreign_policy                   | 0.25       |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | virology                            | 0.198795   |                 0 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | world_religions                     | 0.345029   |                 0 | mosaicml/mpt-7b          |
| world_knowledge          | bigbench_misconceptions          |                                     | 0.479452   |                 0 | mosaicml/mpt-7b          |
| world_knowledge          | bigbench_movie_recommendation    |                                     | 0.262      |                 0 | mosaicml/mpt-7b          |
| commonsense_reasoning    | copa                             |                                     | 0.8        |                 0 | mosaicml/mpt-7b          |
| commonsense_reasoning    | piqa                             |                                     | 0.802503   |                 0 | mosaicml/mpt-7b          |
| commonsense_reasoning    | openbook_qa                      |                                     | 0.418      |                 0 | mosaicml/mpt-7b          |
| commonsense_reasoning    | bigbench_novel_concepts          |                                     | 0.4375     |                 0 | mosaicml/mpt-7b          |
| commonsense_reasoning    | bigbench_strange_stories         |                                     | 0.568965   |                 0 | mosaicml/mpt-7b          |
| commonsense_reasoning    | bigbench_strategy_qa             |                                     | 0.559196   |                 0 | mosaicml/mpt-7b          |
| language_understanding   | lambada_openai                   |                                     | 0.70328    |                 0 | mosaicml/mpt-7b          |
| language_understanding   | hellaswag                        |                                     | 0.761701   |                 0 | mosaicml/mpt-7b          |
| language_understanding   | winograd                         |                                     | 0.868132   |                 0 | mosaicml/mpt-7b          |
| language_understanding   | winogrande                       |                                     | 0.685083   |                 0 | mosaicml/mpt-7b          |
| language_understanding   | bigbench_conlang_translation     |                                     | 0.0670732  |                 0 | mosaicml/mpt-7b          |
| language_understanding   | bigbench_language_identification |                                     | 0.2851     |                 0 | mosaicml/mpt-7b          |
| language_understanding   | bigbench_conceptual_combinations |                                     | 0.359223   |                 0 | mosaicml/mpt-7b          |
| symbolic_problem_solving | bigbench_elementary_math_qa      |                                     | 0.262421   |                 0 | mosaicml/mpt-7b          |
| symbolic_problem_solving | bigbench_dyck_languages          |                                     | 0.056      |                 0 | mosaicml/mpt-7b          |
| symbolic_problem_solving | bigbench_cs_algorithms           |                                     | 0.0325758  |                 0 | mosaicml/mpt-7b          |
| symbolic_problem_solving | bigbench_logical_deduction       |                                     | 0.277333   |                 0 | mosaicml/mpt-7b          |
| symbolic_problem_solving | bigbench_operators               |                                     | 0.2        |                 0 | mosaicml/mpt-7b          |
| symbolic_problem_solving | bigbench_repeat_copy_logic       |                                     | 0          |                 0 | mosaicml/mpt-7b          |
| symbolic_problem_solving | simple_arithmetic_nospaces       |                                     | 0          |                 0 | mosaicml/mpt-7b          |
| symbolic_problem_solving | simple_arithmetic_withspaces     |                                     | 0.008      |                 0 | mosaicml/mpt-7b          |
| symbolic_problem_solving | math_qa                          |                                     | 0.261482   |                 0 | mosaicml/mpt-7b          |
| symbolic_problem_solving | logi_qa                          |                                     | 0.262673   |                 0 | mosaicml/mpt-7b          |
| reading_comprehension    | squad                            |                                     | 0.167928   |                 0 | mosaicml/mpt-7b          |
| reading_comprehension    | coqa                             |                                     | 0.191908   |                 0 | mosaicml/mpt-7b          |
| reading_comprehension    | bigbench_understanding_fables    |                                     | 0.253968   |                 0 | mosaicml/mpt-7b          |
| reading_comprehension    | boolq                            |                                     | 0.770642   |                 0 | mosaicml/mpt-7b          |
| world_knowledge          | jeopardy                         | Average                             | 0.467746   |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | american_history                    | 0.527845   |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | literature                          | 0.579592   |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | science                             | 0.34874    |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | word_origins                        | 0.273973   |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | world_history                       | 0.608579   |                10 | mosaicml/mpt-7b          |
| world_knowledge          | bigbench_qa_wikidata             |                                     | 0.711038   |                10 | mosaicml/mpt-7b          |
| world_knowledge          | arc_easy                         |                                     | 0.738216   |                10 | mosaicml/mpt-7b          |
| world_knowledge          | arc_challenge                    |                                     | 0.438567   |                10 | mosaicml/mpt-7b          |
| world_knowledge          | mmlu                             | Average                             | 0.280097   |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | abstract_algebra                    | 0.3        |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | anatomy                             | 0.244444   |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | astronomy                           | 0.276316   |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | business_ethics                     | 0.29       |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | clinical_knowledge                  | 0.249057   |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | college_biology                     | 0.305556   |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | college_chemistry                   | 0.24       |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | college_computer_science            | 0.28       |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | college_mathematics                 | 0.31       |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | college_medicine                    | 0.254335   |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | college_physics                     | 0.254902   |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | computer_security                   | 0.25       |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | conceptual_physics                  | 0.280851   |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | econometrics                        | 0.210526   |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | electrical_engineering              | 0.262069   |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | elementary_mathematics              | 0.280423   |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | formal_logic                        | 0.293651   |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | global_facts                        | 0.24       |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | high_school_biology                 | 0.267742   |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | high_school_chemistry               | 0.182266   |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | high_school_computer_science        | 0.29       |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | high_school_european_history        | 0.248485   |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | high_school_geography               | 0.277778   |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | high_school_government_and_politics | 0.321244   |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | high_school_macroeconomics          | 0.251282   |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | high_school_mathematics             | 0.277778   |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | high_school_microeconomics          | 0.281513   |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | high_school_physics                 | 0.251656   |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | high_school_psychology              | 0.238532   |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | high_school_statistics              | 0.305556   |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | high_school_us_history              | 0.254902   |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | high_school_world_history           | 0.274262   |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | human_aging                         | 0.367713   |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | human_sexuality                     | 0.274809   |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | international_law                   | 0.330579   |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | jurisprudence                       | 0.314815   |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | logical_fallacies                   | 0.202454   |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | machine_learning                    | 0.357143   |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | management                          | 0.242718   |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | marketing                           | 0.290598   |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | medical_genetics                    | 0.38       |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | miscellaneous                       | 0.291188   |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | moral_disputes                      | 0.32948    |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | moral_scenarios                     | 0.259218   |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | nutrition                           | 0.29085    |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | philosophy                          | 0.299035   |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | prehistory                          | 0.314815   |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | professional_accounting             | 0.258865   |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | professional_law                    | 0.273142   |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | professional_medicine               | 0.216912   |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | professional_psychology             | 0.287582   |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | public_relations                    | 0.290909   |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | security_studies                    | 0.326531   |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | sociology                           | 0.253731   |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | us_foreign_policy                   | 0.32       |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | virology                            | 0.337349   |                10 | mosaicml/mpt-7b          |
| world_knowledge          |                                  | world_religions                     | 0.309942   |                10 | mosaicml/mpt-7b          |
| world_knowledge          | bigbench_misconceptions          |                                     | 0.520548   |                10 | mosaicml/mpt-7b          |
| world_knowledge          | bigbench_movie_recommendation    |                                     | 0.232      |                10 | mosaicml/mpt-7b          |
| commonsense_reasoning    | piqa                             |                                     | 0.807399   |                10 | mosaicml/mpt-7b          |
| commonsense_reasoning    | bigbench_novel_concepts          |                                     | 0.5625     |                10 | mosaicml/mpt-7b          |
| commonsense_reasoning    | bigbench_strange_stories         |                                     | 0.66092    |                10 | mosaicml/mpt-7b          |
| commonsense_reasoning    | bigbench_strategy_qa             |                                     | 0.564875   |                10 | mosaicml/mpt-7b          |
| language_understanding   | hellaswag                        |                                     | 0.765485   |                10 | mosaicml/mpt-7b          |
| language_understanding   | bigbench_conlang_translation     |                                     | 0.0609756  |                10 | mosaicml/mpt-7b          |
| language_understanding   | bigbench_language_identification |                                     | 0.2536     |                10 | mosaicml/mpt-7b          |
| language_understanding   | bigbench_conceptual_combinations |                                     | 0.320388   |                10 | mosaicml/mpt-7b          |
| symbolic_problem_solving | bigbench_elementary_math_qa      |                                     | 0.276494   |                10 | mosaicml/mpt-7b          |
| symbolic_problem_solving | bigbench_dyck_languages          |                                     | 0.318      |                10 | mosaicml/mpt-7b          |
| symbolic_problem_solving | bigbench_cs_algorithms           |                                     | 0.478788   |                10 | mosaicml/mpt-7b          |
| symbolic_problem_solving | bigbench_logical_deduction       |                                     | 0.246      |                10 | mosaicml/mpt-7b          |
| symbolic_problem_solving | bigbench_operators               |                                     | 0.342857   |                10 | mosaicml/mpt-7b          |
| symbolic_problem_solving | bigbench_repeat_copy_logic       |                                     | 0.25       |                10 | mosaicml/mpt-7b          |
| symbolic_problem_solving | simple_arithmetic_nospaces       |                                     | 0.081      |                10 | mosaicml/mpt-7b          |
| symbolic_problem_solving | simple_arithmetic_withspaces     |                                     | 0.092      |                10 | mosaicml/mpt-7b          |
| symbolic_problem_solving | math_qa                          |                                     | 0.263493   |                10 | mosaicml/mpt-7b          |
| symbolic_problem_solving | logi_qa                          |                                     | 0.264209   |                10 | mosaicml/mpt-7b          |
| reading_comprehension    | pubmed_qa_labeled                |                                     | 0.322      |                10 | mosaicml/mpt-7b          |
| reading_comprehension    | squad                            |                                     | 0.583349   |                10 | mosaicml/mpt-7b          |
| reading_comprehension    | coqa                             |                                     | 0.276713   |                10 | mosaicml/mpt-7b          |
| reading_comprehension    | bigbench_understanding_fables    |                                     | 0.206349   |                10 | mosaicml/mpt-7b          |
| reading_comprehension    | boolq                            |                                     | 0.722018   |                10 | mosaicml/mpt-7b          |
| world_knowledge          | jeopardy                         | Average                             |  0.106559  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | american_history                    |  0.130751  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | literature                          |  0.0714286 |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | science                             |  0.0756303 |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | word_origins                        |  0.0136986 |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | world_history                       |  0.241287  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          | triviaqa                         |                                     |  0.233271  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          | bigbench_qa_wikidata             |                                     |  0.431032  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          | arc_easy                         |                                     |  0.680556  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          | arc_challenge                    |                                     |  0.428328  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          | mmlu                             | Average                             |  0.329945  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | abstract_algebra                    |  0.27      |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | anatomy                             |  0.392593  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | astronomy                           |  0.375     |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | business_ethics                     |  0.27      |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | clinical_knowledge                  |  0.388679  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | college_biology                     |  0.291667  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | college_chemistry                   |  0.27      |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | college_computer_science            |  0.38      |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | college_mathematics                 |  0.29      |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | college_medicine                    |  0.364162  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | college_physics                     |  0.22549   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | computer_security                   |  0.36      |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | conceptual_physics                  |  0.289362  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | econometrics                        |  0.184211  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | electrical_engineering              |  0.337931  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | elementary_mathematics              |  0.301587  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | formal_logic                        |  0.285714  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | global_facts                        |  0.31      |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | high_school_biology                 |  0.348387  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | high_school_chemistry               |  0.270936  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | high_school_computer_science        |  0.3       |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | high_school_european_history        |  0.327273  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | high_school_geography               |  0.439394  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | high_school_government_and_politics |  0.430052  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | high_school_macroeconomics          |  0.338462  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | high_school_mathematics             |  0.266667  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | high_school_microeconomics          |  0.361345  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | high_school_physics                 |  0.245033  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | high_school_psychology              |  0.381651  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | high_school_statistics              |  0.226852  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | high_school_us_history              |  0.343137  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | high_school_world_history           |  0.295359  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | human_aging                         |  0.340807  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | human_sexuality                     |  0.320611  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | international_law                   |  0.347107  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | jurisprudence                       |  0.277778  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | logical_fallacies                   |  0.398773  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | machine_learning                    |  0.267857  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | management                          |  0.368932  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | marketing                           |  0.461538  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | medical_genetics                    |  0.34      |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | miscellaneous                       |  0.50447   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | moral_disputes                      |  0.32659   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | moral_scenarios                     |  0.241341  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | nutrition                           |  0.349673  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | philosophy                          |  0.334405  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | prehistory                          |  0.37963   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | professional_accounting             |  0.265957  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | professional_law                    |  0.3103    |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | professional_medicine               |  0.264706  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | professional_psychology             |  0.312091  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | public_relations                    |  0.454545  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | security_studies                    |  0.257143  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | sociology                           |  0.318408  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | us_foreign_policy                   |  0.47      |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | virology                            |  0.277108  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | world_religions                     |  0.45614   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          | bigbench_misconceptions          |                                     |  0.520548  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          | bigbench_movie_recommendation    |                                     |  0.268     |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| commonsense_reasoning    | copa                             |                                     |  0.81      |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| commonsense_reasoning    | piqa                             |                                     |  0.770947  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| commonsense_reasoning    | openbook_qa                      |                                     |  0.408     |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| commonsense_reasoning    | bigbench_novel_concepts          |                                     |  0.34375   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| commonsense_reasoning    | bigbench_strange_stories         |                                     |  0.54023   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| commonsense_reasoning    | bigbench_strategy_qa             |                                     |  0.558322  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| language_understanding   | lambada_openai                   |                                     |  0.687561  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| language_understanding   | hellaswag                        |                                     |  0.703346  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| language_understanding   | winograd                         |                                     |  0.824176  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| language_understanding   | winogrande                       |                                     |  0.651934  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| language_understanding   | bigbench_conlang_translation     |                                     |  0.0426829 |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| language_understanding   | bigbench_language_identification |                                     |  0.2788    |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| language_understanding   | bigbench_conceptual_combinations |                                     |  0.349515  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| symbolic_problem_solving | bigbench_elementary_math_qa      |                                     |  0.261242  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| symbolic_problem_solving | bigbench_dyck_languages          |                                     |  0         |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| symbolic_problem_solving | bigbench_cs_algorithms           |                                     |  0.0272727 |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| symbolic_problem_solving | bigbench_logical_deduction       |                                     |  0.266667  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| symbolic_problem_solving | bigbench_operators               |                                     |  0.147619  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| symbolic_problem_solving | bigbench_repeat_copy_logic       |                                     |  0.125     |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| symbolic_problem_solving | simple_arithmetic_nospaces       |                                     |  0         |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| symbolic_problem_solving | simple_arithmetic_withspaces     |                                     |  0.002     |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| symbolic_problem_solving | math_qa                          |                                     |  0.256788  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| symbolic_problem_solving | logi_qa                          |                                     |  0.298003  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| reading_comprehension    | squad                            |                                     |  0.474551  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| reading_comprehension    | coqa                             |                                     |  0.193536  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| reading_comprehension    | bigbench_understanding_fables    |                                     |  0.375661  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| reading_comprehension    | boolq                            |                                     |  0.65841   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          | jeopardy                         | Average                             |  0.447254  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | american_history                    |  0.506053  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | literature                          |  0.573469  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | science                             |  0.352941  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | word_origins                        |  0.227397  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | world_history                       |  0.576407  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          | bigbench_qa_wikidata             |                                     |  0.733478  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          | arc_easy                         |                                     |  0.729798  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          | arc_challenge                    |                                     |  0.445392  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          | mmlu                             | Average                             |  0.371999  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | abstract_algebra                    |  0.3       |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | anatomy                             |  0.451852  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | astronomy                           |  0.401316  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | business_ethics                     |  0.39      |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | clinical_knowledge                  |  0.445283  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | college_biology                     |  0.354167  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | college_chemistry                   |  0.3       |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | college_computer_science            |  0.34      |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | college_mathematics                 |  0.34      |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | college_medicine                    |  0.352601  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | college_physics                     |  0.284314  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | computer_security                   |  0.52      |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | conceptual_physics                  |  0.314894  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | econometrics                        |  0.166667  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | electrical_engineering              |  0.296552  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | elementary_mathematics              |  0.26455   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | formal_logic                        |  0.238095  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | global_facts                        |  0.34      |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | high_school_biology                 |  0.435484  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | high_school_chemistry               |  0.300493  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | high_school_computer_science        |  0.35      |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | high_school_european_history        |  0.466667  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | high_school_geography               |  0.424242  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | high_school_government_and_politics |  0.497409  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | high_school_macroeconomics          |  0.376923  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | high_school_mathematics             |  0.251852  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | high_school_microeconomics          |  0.336134  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | high_school_physics                 |  0.271523  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | high_school_psychology              |  0.456881  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | high_school_statistics              |  0.208333  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | high_school_us_history              |  0.416667  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | high_school_world_history           |  0.464135  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | human_aging                         |  0.488789  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | human_sexuality                     |  0.374046  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | international_law                   |  0.504132  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | jurisprudence                       |  0.435185  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | logical_fallacies                   |  0.398773  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | machine_learning                    |  0.25      |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | management                          |  0.368932  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | marketing                           |  0.521368  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | medical_genetics                    |  0.38      |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | miscellaneous                       |  0.526181  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | moral_disputes                      |  0.410405  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | moral_scenarios                     |  0.250279  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | nutrition                           |  0.379085  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | philosophy                          |  0.398714  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | prehistory                          |  0.429012  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | professional_accounting             |  0.326241  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | professional_law                    |  0.342243  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | professional_medicine               |  0.272059  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | professional_psychology             |  0.372549  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | public_relations                    |  0.418182  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | security_studies                    |  0.269388  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | sociology                           |  0.40796   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | us_foreign_policy                   |  0.49      |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | virology                            |  0.301205  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | world_religions                     |  0.532164  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          | bigbench_misconceptions          |                                     |  0.543379  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          | bigbench_movie_recommendation    |                                     |  0.54      |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| commonsense_reasoning    | piqa                             |                                     |  0.780196  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| commonsense_reasoning    | bigbench_novel_concepts          |                                     |  0.5625    |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| commonsense_reasoning    | bigbench_strange_stories         |                                     |  0.637931  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| commonsense_reasoning    | bigbench_strategy_qa             |                                     |  0.559633  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| language_understanding   | hellaswag                        |                                     |  0.710018  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| language_understanding   | bigbench_conlang_translation     |                                     |  0.0365854 |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| language_understanding   | bigbench_language_identification |                                     |  0.3032    |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| language_understanding   | bigbench_conceptual_combinations |                                     |  0.553398  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| symbolic_problem_solving | bigbench_elementary_math_qa      |                                     |  0.256315  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| symbolic_problem_solving | bigbench_dyck_languages          |                                     |  0.204     |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| symbolic_problem_solving | bigbench_cs_algorithms           |                                     |  0.483333  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| symbolic_problem_solving | bigbench_logical_deduction       |                                     |  0.265333  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| symbolic_problem_solving | bigbench_operators               |                                     |  0.304762  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| symbolic_problem_solving | bigbench_repeat_copy_logic       |                                     |  0.90625   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| symbolic_problem_solving | simple_arithmetic_nospaces       |                                     |  0.044     |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| symbolic_problem_solving | simple_arithmetic_withspaces     |                                     |  0.038     |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| symbolic_problem_solving | math_qa                          |                                     |  0.26584   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| symbolic_problem_solving | logi_qa                          |                                     |  0.302611  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| reading_comprehension    | pubmed_qa_labeled                |                                     |  0.635     |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| reading_comprehension    | squad                            |                                     |  0.575686  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| reading_comprehension    | coqa                             |                                     |  0.265439  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| reading_comprehension    | bigbench_understanding_fables    |                                     |  0.412698  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| reading_comprehension    | boolq                            |                                     |  0.735474  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          | jeopardy                         | Average                             | 0.106559   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | american_history                    | 0.130751   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | literature                          | 0.0714286  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | science                             | 0.0756303  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | word_origins                        | 0.0136986  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | world_history                       | 0.241287   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          | triviaqa                         |                                     | 0.233271   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          | bigbench_qa_wikidata             |                                     | 0.431032   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          | arc_easy                         |                                     | 0.680556   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          | arc_challenge                    |                                     | 0.428328   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          | mmlu                             | Average                             | 0.329945   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | abstract_algebra                    | 0.27       |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | anatomy                             | 0.392593   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | astronomy                           | 0.375      |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | business_ethics                     | 0.27       |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | clinical_knowledge                  | 0.388679   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | college_biology                     | 0.291667   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | college_chemistry                   | 0.27       |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | college_computer_science            | 0.38       |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | college_mathematics                 | 0.29       |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | college_medicine                    | 0.364162   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | college_physics                     | 0.22549    |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | computer_security                   | 0.36       |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | conceptual_physics                  | 0.289362   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | econometrics                        | 0.184211   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | electrical_engineering              | 0.337931   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | elementary_mathematics              | 0.301587   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | formal_logic                        | 0.285714   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | global_facts                        | 0.31       |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | high_school_biology                 | 0.348387   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | high_school_chemistry               | 0.270936   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | high_school_computer_science        | 0.3        |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | high_school_european_history        | 0.327273   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | high_school_geography               | 0.439394   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | high_school_government_and_politics | 0.430052   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | high_school_macroeconomics          | 0.338462   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | high_school_mathematics             | 0.266667   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | high_school_microeconomics          | 0.361345   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | high_school_physics                 | 0.245033   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | high_school_psychology              | 0.381651   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | high_school_statistics              | 0.226852   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | high_school_us_history              | 0.343137   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | high_school_world_history           | 0.295359   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | human_aging                         | 0.340807   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | human_sexuality                     | 0.320611   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | international_law                   | 0.347107   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | jurisprudence                       | 0.277778   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | logical_fallacies                   | 0.398773   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | machine_learning                    | 0.267857   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | management                          | 0.368932   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | marketing                           | 0.461538   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | medical_genetics                    | 0.34       |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | miscellaneous                       | 0.50447    |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | moral_disputes                      | 0.32659    |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | moral_scenarios                     | 0.241341   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | nutrition                           | 0.349673   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | philosophy                          | 0.334405   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | prehistory                          | 0.37963    |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | professional_accounting             | 0.265957   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | professional_law                    | 0.3103     |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | professional_medicine               | 0.264706   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | professional_psychology             | 0.312091   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | public_relations                    | 0.454545   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | security_studies                    | 0.257143   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | sociology                           | 0.318408   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | us_foreign_policy                   | 0.47       |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | virology                            | 0.277108   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | world_religions                     | 0.45614    |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          | bigbench_misconceptions          |                                     | 0.520548   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          | bigbench_movie_recommendation    |                                     | 0.268      |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| commonsense_reasoning    | copa                             |                                     | 0.81       |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| commonsense_reasoning    | piqa                             |                                     | 0.770947   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| commonsense_reasoning    | openbook_qa                      |                                     | 0.408      |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| commonsense_reasoning    | bigbench_novel_concepts          |                                     | 0.34375    |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| commonsense_reasoning    | bigbench_strange_stories         |                                     | 0.54023    |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| commonsense_reasoning    | bigbench_strategy_qa             |                                     | 0.558322   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| language_understanding   | lambada_openai                   |                                     | 0.687561   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| language_understanding   | hellaswag                        |                                     | 0.703346   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| language_understanding   | winograd                         |                                     | 0.824176   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| language_understanding   | winogrande                       |                                     | 0.651934   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| language_understanding   | bigbench_conlang_translation     |                                     | 0.0426829  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| language_understanding   | bigbench_language_identification |                                     | 0.2788     |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| language_understanding   | bigbench_conceptual_combinations |                                     | 0.349515   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| symbolic_problem_solving | bigbench_elementary_math_qa      |                                     | 0.261242   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| symbolic_problem_solving | bigbench_dyck_languages          |                                     | 0          |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| symbolic_problem_solving | bigbench_cs_algorithms           |                                     | 0.0272727  |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| symbolic_problem_solving | bigbench_logical_deduction       |                                     | 0.266667   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| symbolic_problem_solving | bigbench_operators               |                                     | 0.147619   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| symbolic_problem_solving | bigbench_repeat_copy_logic       |                                     | 0.125      |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| symbolic_problem_solving | simple_arithmetic_nospaces       |                                     | 0          |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| symbolic_problem_solving | simple_arithmetic_withspaces     |                                     | 0.002      |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| symbolic_problem_solving | math_qa                          |                                     | 0.256788   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| symbolic_problem_solving | logi_qa                          |                                     | 0.298003   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| reading_comprehension    | squad                            |                                     | 0.474551   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| reading_comprehension    | coqa                             |                                     | 0.193536   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| reading_comprehension    | bigbench_understanding_fables    |                                     | 0.375661   |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| reading_comprehension    | boolq                            |                                     | 0.65841    |                 0 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          | jeopardy                         | Average                             | 0.447254   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | american_history                    | 0.506053   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | literature                          | 0.573469   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | science                             | 0.352941   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | word_origins                        | 0.227397   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | world_history                       | 0.576407   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          | bigbench_qa_wikidata             |                                     | 0.733478   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          | arc_easy                         |                                     | 0.729798   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          | arc_challenge                    |                                     | 0.445392   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          | mmlu                             | Average                             | 0.371999   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | abstract_algebra                    | 0.3        |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | anatomy                             | 0.451852   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | astronomy                           | 0.401316   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | business_ethics                     | 0.39       |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | clinical_knowledge                  | 0.445283   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | college_biology                     | 0.354167   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | college_chemistry                   | 0.3        |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | college_computer_science            | 0.34       |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | college_mathematics                 | 0.34       |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | college_medicine                    | 0.352601   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | college_physics                     | 0.284314   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | computer_security                   | 0.52       |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | conceptual_physics                  | 0.314894   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | econometrics                        | 0.166667   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | electrical_engineering              | 0.296552   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | elementary_mathematics              | 0.26455    |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | formal_logic                        | 0.238095   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | global_facts                        | 0.34       |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | high_school_biology                 | 0.435484   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | high_school_chemistry               | 0.300493   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | high_school_computer_science        | 0.35       |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | high_school_european_history        | 0.466667   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | high_school_geography               | 0.424242   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | high_school_government_and_politics | 0.497409   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | high_school_macroeconomics          | 0.376923   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | high_school_mathematics             | 0.251852   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | high_school_microeconomics          | 0.336134   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | high_school_physics                 | 0.271523   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | high_school_psychology              | 0.456881   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | high_school_statistics              | 0.208333   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | high_school_us_history              | 0.416667   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | high_school_world_history           | 0.464135   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | human_aging                         | 0.488789   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | human_sexuality                     | 0.374046   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | international_law                   | 0.504132   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | jurisprudence                       | 0.435185   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | logical_fallacies                   | 0.398773   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | machine_learning                    | 0.25       |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | management                          | 0.368932   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | marketing                           | 0.521368   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | medical_genetics                    | 0.38       |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | miscellaneous                       | 0.526181   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | moral_disputes                      | 0.410405   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | moral_scenarios                     | 0.250279   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | nutrition                           | 0.379085   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | philosophy                          | 0.398714   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | prehistory                          | 0.429012   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | professional_accounting             | 0.326241   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | professional_law                    | 0.342243   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | professional_medicine               | 0.272059   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | professional_psychology             | 0.372549   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | public_relations                    | 0.418182   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | security_studies                    | 0.269388   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | sociology                           | 0.40796    |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | us_foreign_policy                   | 0.49       |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | virology                            | 0.301205   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          |                                  | world_religions                     | 0.532164   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          | bigbench_misconceptions          |                                     | 0.543379   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          | bigbench_movie_recommendation    |                                     | 0.54       |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| commonsense_reasoning    | piqa                             |                                     | 0.780196   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| commonsense_reasoning    | bigbench_novel_concepts          |                                     | 0.5625     |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| commonsense_reasoning    | bigbench_strange_stories         |                                     | 0.637931   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| commonsense_reasoning    | bigbench_strategy_qa             |                                     | 0.559633   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| language_understanding   | hellaswag                        |                                     | 0.710018   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| language_understanding   | bigbench_conlang_translation     |                                     | 0.0365854  |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| language_understanding   | bigbench_language_identification |                                     | 0.3032     |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| language_understanding   | bigbench_conceptual_combinations |                                     | 0.553398   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| symbolic_problem_solving | bigbench_elementary_math_qa      |                                     | 0.256315   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| symbolic_problem_solving | bigbench_dyck_languages          |                                     | 0.204      |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| symbolic_problem_solving | bigbench_cs_algorithms           |                                     | 0.483333   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| symbolic_problem_solving | bigbench_logical_deduction       |                                     | 0.265333   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| symbolic_problem_solving | bigbench_operators               |                                     | 0.304762   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| symbolic_problem_solving | bigbench_repeat_copy_logic       |                                     | 0.90625    |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| symbolic_problem_solving | simple_arithmetic_nospaces       |                                     | 0.044      |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| symbolic_problem_solving | simple_arithmetic_withspaces     |                                     | 0.038      |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| symbolic_problem_solving | math_qa                          |                                     | 0.26584    |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| symbolic_problem_solving | logi_qa                          |                                     | 0.302611   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| reading_comprehension    | pubmed_qa_labeled                |                                     | 0.635      |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| reading_comprehension    | squad                            |                                     | 0.575686   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| reading_comprehension    | coqa                             |                                     | 0.265439   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| reading_comprehension    | bigbench_understanding_fables    |                                     | 0.412698   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| reading_comprehension    | boolq                            |                                     | 0.735474   |                10 | togethercomputer/RedPajama-INCITE-7B-Instruct |
| world_knowledge          | jeopardy                         | Average                             | 0.0992664  |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | american_history                    | 0.157385   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | literature                          | 0.0734694  |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | science                             | 0.0483193  |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | word_origins                        | 0          |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | world_history                       | 0.217158   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          | triviaqa                         |                                     | 0.41165    |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          | bigbench_qa_wikidata             |                                     | 0.396339   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          | arc_easy                         |                                     | 0.641414   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          | arc_challenge                    |                                     | 0.395904   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          | mmlu                             | Average                             | 0.250243   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | abstract_algebra                    | 0.31       |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | anatomy                             | 0.259259   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | astronomy                           | 0.157895   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | business_ethics                     | 0.27       |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | clinical_knowledge                  | 0.207547   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | college_biology                     | 0.291667   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | college_chemistry                   | 0.19       |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | college_computer_science            | 0.2        |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | college_mathematics                 | 0.29       |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | college_medicine                    | 0.225434   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | college_physics                     | 0.245098   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | computer_security                   | 0.28       |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | conceptual_physics                  | 0.242553   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | econometrics                        | 0.27193    |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | electrical_engineering              | 0.234483   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | elementary_mathematics              | 0.248677   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | formal_logic                        | 0.261905   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | global_facts                        | 0.25       |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | high_school_biology                 | 0.206452   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | high_school_chemistry               | 0.192118   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | high_school_computer_science        | 0.34       |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | high_school_european_history        | 0.254545   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | high_school_geography               | 0.156566   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | high_school_government_and_politics | 0.207254   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | high_school_macroeconomics          | 0.235897   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | high_school_mathematics             | 0.266667   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | high_school_microeconomics          | 0.247899   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | high_school_physics                 | 0.251656   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | high_school_psychology              | 0.233028   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | high_school_statistics              | 0.222222   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | high_school_us_history              | 0.308824   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | high_school_world_history           | 0.261603   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | human_aging                         | 0.29148    |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | human_sexuality                     | 0.259542   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | international_law                   | 0.247934   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | jurisprudence                       | 0.314815   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | logical_fallacies                   | 0.233129   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | machine_learning                    | 0.303571   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | management                          | 0.213592   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | marketing                           | 0.269231   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | medical_genetics                    | 0.25       |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | miscellaneous                       | 0.275862   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | moral_disputes                      | 0.283237   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | moral_scenarios                     | 0.249162   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | nutrition                           | 0.27451    |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | philosophy                          | 0.231511   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | prehistory                          | 0.253086   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | professional_accounting             | 0.230496   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | professional_law                    | 0.250326   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | professional_medicine               | 0.191176   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | professional_psychology             | 0.251634   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | public_relations                    | 0.3        |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | security_studies                    | 0.204082   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | sociology                           | 0.253731   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | us_foreign_policy                   | 0.26       |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | virology                            | 0.228916   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | world_religions                     | 0.321637   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          | bigbench_misconceptions          |                                     | 0.447489   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          | bigbench_movie_recommendation    |                                     | 0.256      |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| commonsense_reasoning    | copa                             |                                     | 0.81       |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| commonsense_reasoning    | piqa                             |                                     | 0.775843   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| commonsense_reasoning    | openbook_qa                      |                                     | 0.416      |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| commonsense_reasoning    | bigbench_novel_concepts          |                                     | 0.375      |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| commonsense_reasoning    | bigbench_strange_stories         |                                     | 0.563218   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| commonsense_reasoning    | bigbench_strategy_qa             |                                     | 0.546964   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| language_understanding   | lambada_openai                   |                                     | 0.713565   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| language_understanding   | hellaswag                        |                                     | 0.704342   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| language_understanding   | winograd                         |                                     | 0.868132   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| language_understanding   | winogrande                       |                                     | 0.643252   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| language_understanding   | bigbench_conlang_translation     |                                     | 0.0609756  |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| language_understanding   | bigbench_language_identification |                                     | 0.2492     |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| language_understanding   | bigbench_conceptual_combinations |                                     | 0.281553   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| symbolic_problem_solving | bigbench_elementary_math_qa      |                                     | 0.267086   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| symbolic_problem_solving | bigbench_dyck_languages          |                                     | 0          |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| symbolic_problem_solving | bigbench_cs_algorithms           |                                     | 0.0265152  |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| symbolic_problem_solving | bigbench_logical_deduction       |                                     | 0.257333   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| symbolic_problem_solving | bigbench_operators               |                                     | 0.109524   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| symbolic_problem_solving | bigbench_repeat_copy_logic       |                                     | 0          |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| symbolic_problem_solving | simple_arithmetic_nospaces       |                                     | 0          |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| symbolic_problem_solving | simple_arithmetic_withspaces     |                                     | 0.006      |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| symbolic_problem_solving | math_qa                          |                                     | 0.261817   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| symbolic_problem_solving | logi_qa                          |                                     | 0.25192    |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| reading_comprehension    | squad                            |                                     | 0.165279   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| reading_comprehension    | coqa                             |                                     | 0.161343   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| reading_comprehension    | bigbench_understanding_fables    |                                     | 0.26455    |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| reading_comprehension    | boolq                            |                                     | 0.709174   |                 0 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          | jeopardy                         | Average                             | 0.468312   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | american_history                    | 0.520581   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | literature                          | 0.612245   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | science                             | 0.35084    |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | word_origins                        | 0.249315   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | world_history                       | 0.608579   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          | bigbench_qa_wikidata             |                                     | 0.725752   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          | arc_easy                         |                                     | 0.729377   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          | arc_challenge                    |                                     | 0.432594   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          | mmlu                             | Average                             | 0.262514   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | abstract_algebra                    | 0.32       |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | anatomy                             | 0.22963    |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | astronomy                           | 0.342105   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | business_ethics                     | 0.2        |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | clinical_knowledge                  | 0.267925   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | college_biology                     | 0.270833   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | college_chemistry                   | 0.28       |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | college_computer_science            | 0.2        |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | college_mathematics                 | 0.34       |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | college_medicine                    | 0.289017   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | college_physics                     | 0.264706   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | computer_security                   | 0.25       |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | conceptual_physics                  | 0.182979   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | econometrics                        | 0.236842   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | electrical_engineering              | 0.282759   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | elementary_mathematics              | 0.256614   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | formal_logic                        | 0.246032   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | global_facts                        | 0.23       |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | high_school_biology                 | 0.267742   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | high_school_chemistry               | 0.251232   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | high_school_computer_science        | 0.27       |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | high_school_european_history        | 0.30303    |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | high_school_geography               | 0.338384   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | high_school_government_and_politics | 0.279793   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | high_school_macroeconomics          | 0.253846   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | high_school_mathematics             | 0.277778   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | high_school_microeconomics          | 0.264706   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | high_school_physics                 | 0.317881   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | high_school_psychology              | 0.26422    |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | high_school_statistics              | 0.310185   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | high_school_us_history              | 0.230392   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | high_school_world_history           | 0.232068   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | human_aging                         | 0.246637   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | human_sexuality                     | 0.221374   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | international_law                   | 0.31405    |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | jurisprudence                       | 0.185185   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | logical_fallacies                   | 0.263804   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | machine_learning                    | 0.241071   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | management                          | 0.15534    |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | marketing                           | 0.235043   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | medical_genetics                    | 0.28       |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | miscellaneous                       | 0.247765   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | moral_disputes                      | 0.271676   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | moral_scenarios                     | 0.272626   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | nutrition                           | 0.29085    |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | philosophy                          | 0.318328   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | prehistory                          | 0.339506   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | professional_accounting             | 0.234043   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | professional_law                    | 0.243807   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | professional_medicine               | 0.205882   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | professional_psychology             | 0.238562   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | public_relations                    | 0.209091   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | security_studies                    | 0.314286   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | sociology                           | 0.278607   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | us_foreign_policy                   | 0.26       |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | virology                            | 0.240964   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          |                                  | world_religions                     | 0.304094   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          | bigbench_misconceptions          |                                     | 0.502283   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          | bigbench_movie_recommendation    |                                     | 0.24       |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| commonsense_reasoning    | piqa                             |                                     | 0.785092   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| commonsense_reasoning    | bigbench_novel_concepts          |                                     | 0.53125    |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| commonsense_reasoning    | bigbench_strange_stories         |                                     | 0.66092    |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| commonsense_reasoning    | bigbench_strategy_qa             |                                     | 0.535605   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| language_understanding   | hellaswag                        |                                     | 0.717785   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| language_understanding   | bigbench_conlang_translation     |                                     | 0.0487805  |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| language_understanding   | bigbench_language_identification |                                     | 0.246      |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| language_understanding   | bigbench_conceptual_combinations |                                     | 0.398058   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| symbolic_problem_solving | bigbench_elementary_math_qa      |                                     | 0.256918   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| symbolic_problem_solving | bigbench_dyck_languages          |                                     | 0.329      |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| symbolic_problem_solving | bigbench_cs_algorithms           |                                     | 0.467424   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| symbolic_problem_solving | bigbench_logical_deduction       |                                     | 0.24       |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| symbolic_problem_solving | bigbench_operators               |                                     | 0.271429   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| symbolic_problem_solving | bigbench_repeat_copy_logic       |                                     | 0.09375    |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| symbolic_problem_solving | simple_arithmetic_nospaces       |                                     | 0.042      |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| symbolic_problem_solving | simple_arithmetic_withspaces     |                                     | 0.034      |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| symbolic_problem_solving | math_qa                          |                                     | 0.258465   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| symbolic_problem_solving | logi_qa                          |                                     | 0.265745   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| reading_comprehension    | pubmed_qa_labeled                |                                     | 0.574      |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| reading_comprehension    | squad                            |                                     | 0.509839   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| reading_comprehension    | coqa                             |                                     | 0.264061   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| reading_comprehension    | bigbench_understanding_fables    |                                     | 0.243386   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| reading_comprehension    | boolq                            |                                     | 0.609786   |                10 | togethercomputer/RedPajama-INCITE-7B-Base     |
| world_knowledge          | jeopardy                         | Average                             | 0.0831953  |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | american_history                    | 0.0823245  |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | literature                          | 0.0591837  |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | science                             | 0.0567227  |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | word_origins                        | 0.0273973  |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | world_history                       | 0.190349   |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          | triviaqa                         |                                     | 0.00114912 |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          | bigbench_qa_wikidata             |                                     | 0.377885   |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          | arc_easy                         |                                     | 0.658249   |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          | arc_challenge                    |                                     | 0.408703   |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          | mmlu                             | Average                             | 0.262599   |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | abstract_algebra                    | 0.25       |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | anatomy                             | 0.274074   |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | astronomy                           | 0.203947   |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | business_ethics                     | 0.29       |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | clinical_knowledge                  | 0.298113   |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | college_biology                     | 0.263889   |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | college_chemistry                   | 0.23       |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | college_computer_science            | 0.17       |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | college_mathematics                 | 0.28       |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | college_medicine                    | 0.248555   |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | college_physics                     | 0.254902   |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | computer_security                   | 0.26       |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | conceptual_physics                  | 0.314894   |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | econometrics                        | 0.298246   |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | electrical_engineering              | 0.248276   |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | elementary_mathematics              | 0.248677   |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | formal_logic                        | 0.277778   |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | global_facts                        | 0.31       |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | high_school_biology                 | 0.232258   |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | high_school_chemistry               | 0.280788   |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | high_school_computer_science        | 0.24       |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | high_school_european_history        | 0.242424   |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | high_school_geography               | 0.267677   |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | high_school_government_and_politics | 0.227979   |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | high_school_macroeconomics          | 0.258974   |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | high_school_mathematics             | 0.277778   |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | high_school_microeconomics          | 0.268908   |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | high_school_physics                 | 0.172185   |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | high_school_psychology              | 0.238532   |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | high_school_statistics              | 0.162037   |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | high_school_us_history              | 0.259804   |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | high_school_world_history           | 0.253165   |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | human_aging                         | 0.38565    |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | human_sexuality                     | 0.244275   |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | international_law                   | 0.289256   |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | jurisprudence                       | 0.314815   |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | logical_fallacies                   | 0.251534   |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | machine_learning                    | 0.294643   |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | management                          | 0.320388   |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | marketing                           | 0.252137   |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | medical_genetics                    | 0.3        |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | miscellaneous                       | 0.279693   |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | moral_disputes                      | 0.257225   |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | moral_scenarios                     | 0.251397   |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | nutrition                           | 0.294118   |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | philosophy                          | 0.221865   |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | prehistory                          | 0.287037   |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | professional_accounting             | 0.244681   |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | professional_law                    | 0.237288   |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | professional_medicine               | 0.1875     |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | professional_psychology             | 0.243464   |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | public_relations                    | 0.363636   |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | security_studies                    | 0.191837   |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | sociology                           | 0.258706   |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | us_foreign_policy                   | 0.31       |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | virology                            | 0.343374   |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | world_religions                     | 0.239766   |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          | bigbench_misconceptions          |                                     | 0.488584   |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          | bigbench_movie_recommendation    |                                     | 0.242      |                 0 | tiiuae/falcon-7b-instruct                     |
| commonsense_reasoning    | copa                             |                                     | 0.77       |                 0 | tiiuae/falcon-7b-instruct                     |
| commonsense_reasoning    | piqa                             |                                     | 0.784004   |                 0 | tiiuae/falcon-7b-instruct                     |
| commonsense_reasoning    | openbook_qa                      |                                     | 0.416      |                 0 | tiiuae/falcon-7b-instruct                     |
| commonsense_reasoning    | bigbench_novel_concepts          |                                     | 0.4375     |                 0 | tiiuae/falcon-7b-instruct                     |
| commonsense_reasoning    | bigbench_strange_stories         |                                     | 0.678161   |                 0 | tiiuae/falcon-7b-instruct                     |
| commonsense_reasoning    | bigbench_strategy_qa             |                                     | 0.558759   |                 0 | tiiuae/falcon-7b-instruct                     |
| language_understanding   | lambada_openai                   |                                     | 0.643703   |                 0 | tiiuae/falcon-7b-instruct                     |
| language_understanding   | hellaswag                        |                                     | 0.702948   |                 0 | tiiuae/falcon-7b-instruct                     |
| language_understanding   | winograd                         |                                     | 0.831502   |                 0 | tiiuae/falcon-7b-instruct                     |
| language_understanding   | winogrande                       |                                     | 0.66693    |                 0 | tiiuae/falcon-7b-instruct                     |
| language_understanding   | bigbench_conlang_translation     |                                     | 0.0243902  |                 0 | tiiuae/falcon-7b-instruct                     |
| language_understanding   | bigbench_language_identification |                                     | 0.2546     |                 0 | tiiuae/falcon-7b-instruct                     |
| language_understanding   | bigbench_conceptual_combinations |                                     | 0.330097   |                 0 | tiiuae/falcon-7b-instruct                     |
| symbolic_problem_solving | bigbench_elementary_math_qa      |                                     | 0.255713   |                 0 | tiiuae/falcon-7b-instruct                     |
| symbolic_problem_solving | bigbench_dyck_languages          |                                     | 0          |                 0 | tiiuae/falcon-7b-instruct                     |
| symbolic_problem_solving | bigbench_cs_algorithms           |                                     | 0.0280303  |                 0 | tiiuae/falcon-7b-instruct                     |
| symbolic_problem_solving | bigbench_logical_deduction       |                                     | 0.277333   |                 0 | tiiuae/falcon-7b-instruct                     |
| symbolic_problem_solving | bigbench_operators               |                                     | 0.280952   |                 0 | tiiuae/falcon-7b-instruct                     |
| symbolic_problem_solving | bigbench_repeat_copy_logic       |                                     | 0          |                 0 | tiiuae/falcon-7b-instruct                     |
| symbolic_problem_solving | simple_arithmetic_nospaces       |                                     | 0.002      |                 0 | tiiuae/falcon-7b-instruct                     |
| symbolic_problem_solving | simple_arithmetic_withspaces     |                                     | 0.018      |                 0 | tiiuae/falcon-7b-instruct                     |
| symbolic_problem_solving | math_qa                          |                                     | 0.254442   |                 0 | tiiuae/falcon-7b-instruct                     |
| symbolic_problem_solving | logi_qa                          |                                     | 0.219662   |                 0 | tiiuae/falcon-7b-instruct                     |
| reading_comprehension    | squad                            |                                     | 0.179754   |                 0 | tiiuae/falcon-7b-instruct                     |
| reading_comprehension    | coqa                             |                                     | 0.150445   |                 0 | tiiuae/falcon-7b-instruct                     |
| reading_comprehension    | bigbench_understanding_fables    |                                     | 0.26455    |                 0 | tiiuae/falcon-7b-instruct                     |
| reading_comprehension    | boolq                            |                                     | 0.713456   |                 0 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          | jeopardy                         | Average                             | 0.311622   |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | american_history                    | 0.355932   |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | literature                          | 0.361225   |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | science                             | 0.256303   |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | word_origins                        | 0.134247   |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | world_history                       | 0.450402   |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          | bigbench_qa_wikidata             |                                     | 0.643817   |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          | arc_easy                         |                                     | 0.712121   |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          | arc_challenge                    |                                     | 0.430034   |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          | mmlu                             | Average                             | 0.259721   |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | abstract_algebra                    | 0.26       |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | anatomy                             | 0.177778   |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | astronomy                           | 0.230263   |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | business_ethics                     | 0.27       |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | clinical_knowledge                  | 0.25283    |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | college_biology                     | 0.25       |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | college_chemistry                   | 0.23       |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | college_computer_science            | 0.33       |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | college_mathematics                 | 0.25       |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | college_medicine                    | 0.283237   |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | college_physics                     | 0.235294   |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | computer_security                   | 0.35       |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | conceptual_physics                  | 0.293617   |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | econometrics                        | 0.236842   |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | electrical_engineering              | 0.234483   |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | elementary_mathematics              | 0.253968   |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | formal_logic                        | 0.349206   |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | global_facts                        | 0.22       |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | high_school_biology                 | 0.23871    |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | high_school_chemistry               | 0.167488   |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | high_school_computer_science        | 0.25       |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | high_school_european_history        | 0.266667   |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | high_school_geography               | 0.237374   |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | high_school_government_and_politics | 0.248705   |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | high_school_macroeconomics          | 0.230769   |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | high_school_mathematics             | 0.174074   |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | high_school_microeconomics          | 0.222689   |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | high_school_physics                 | 0.198675   |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | high_school_psychology              | 0.247706   |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | high_school_statistics              | 0.203704   |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | high_school_us_history              | 0.254902   |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | high_school_world_history           | 0.2827     |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | human_aging                         | 0.367713   |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | human_sexuality                     | 0.305344   |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | international_law                   | 0.247934   |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | jurisprudence                       | 0.268519   |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | logical_fallacies                   | 0.184049   |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | machine_learning                    | 0.330357   |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | management                          | 0.184466   |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | marketing                           | 0.311966   |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | medical_genetics                    | 0.34       |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | miscellaneous                       | 0.250319   |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | moral_disputes                      | 0.271676   |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | moral_scenarios                     | 0.250279   |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | nutrition                           | 0.300654   |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | philosophy                          | 0.22508    |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | prehistory                          | 0.246914   |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | professional_accounting             | 0.283688   |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | professional_law                    | 0.241199   |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | professional_medicine               | 0.220588   |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | professional_psychology             | 0.253268   |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | public_relations                    | 0.263636   |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | security_studies                    | 0.297959   |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | sociology                           | 0.273632   |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | us_foreign_policy                   | 0.33       |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | virology                            | 0.313253   |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          |                                  | world_religions                     | 0.309942   |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          | bigbench_misconceptions          |                                     | 0.593607   |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          | bigbench_movie_recommendation    |                                     | 0.302      |                10 | tiiuae/falcon-7b-instruct                     |
| commonsense_reasoning    | piqa                             |                                     | 0.78346    |                10 | tiiuae/falcon-7b-instruct                     |
| commonsense_reasoning    | bigbench_novel_concepts          |                                     | 0.5625     |                10 | tiiuae/falcon-7b-instruct                     |
| commonsense_reasoning    | bigbench_strange_stories         |                                     | 0.66092    |                10 | tiiuae/falcon-7b-instruct                     |
| commonsense_reasoning    | bigbench_strategy_qa             |                                     | 0.577545   |                10 | tiiuae/falcon-7b-instruct                     |
| language_understanding   | hellaswag                        |                                     | 0.698168   |                10 | tiiuae/falcon-7b-instruct                     |
| language_understanding   | bigbench_conlang_translation     |                                     | 0.0304878  |                10 | tiiuae/falcon-7b-instruct                     |
| language_understanding   | bigbench_language_identification |                                     | 0.2576     |                10 | tiiuae/falcon-7b-instruct                     |
| language_understanding   | bigbench_conceptual_combinations |                                     | 0.291262   |                10 | tiiuae/falcon-7b-instruct                     |
| symbolic_problem_solving | bigbench_elementary_math_qa      |                                     | 0.265514   |                10 | tiiuae/falcon-7b-instruct                     |
| symbolic_problem_solving | bigbench_dyck_languages          |                                     | 0.186      |                10 | tiiuae/falcon-7b-instruct                     |
| symbolic_problem_solving | bigbench_cs_algorithms           |                                     | 0.476515   |                10 | tiiuae/falcon-7b-instruct                     |
| symbolic_problem_solving | bigbench_logical_deduction       |                                     | 0.267333   |                10 | tiiuae/falcon-7b-instruct                     |
| symbolic_problem_solving | bigbench_operators               |                                     | 0.252381   |                10 | tiiuae/falcon-7b-instruct                     |
| symbolic_problem_solving | bigbench_repeat_copy_logic       |                                     | 0.03125    |                10 | tiiuae/falcon-7b-instruct                     |
| symbolic_problem_solving | simple_arithmetic_nospaces       |                                     | 0.051      |                10 | tiiuae/falcon-7b-instruct                     |
| symbolic_problem_solving | simple_arithmetic_withspaces     |                                     | 0.077      |                10 | tiiuae/falcon-7b-instruct                     |
| symbolic_problem_solving | math_qa                          |                                     | 0.253101   |                10 | tiiuae/falcon-7b-instruct                     |
| symbolic_problem_solving | logi_qa                          |                                     | 0.218126   |                10 | tiiuae/falcon-7b-instruct                     |
| reading_comprehension    | pubmed_qa_labeled                |                                     | 0.61       |                10 | tiiuae/falcon-7b-instruct                     |
| reading_comprehension    | squad                            |                                     | 0.453359   |                10 | tiiuae/falcon-7b-instruct                     |
| reading_comprehension    | coqa                             |                                     | 0.212577   |                10 | tiiuae/falcon-7b-instruct                     |
| reading_comprehension    | bigbench_understanding_fables    |                                     | 0.280423   |                10 | tiiuae/falcon-7b-instruct                     |
| reading_comprehension    | boolq                            |                                     | 0.688685   |                10 | tiiuae/falcon-7b-instruct                     |
| world_knowledge          | jeopardy                         | Average                             | 0.328749   |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | american_history                    | 0.438257   |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | literature                          | 0.377551   |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | science                             | 0.203782   |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | word_origins                        | 0.10137    |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | world_history                       | 0.522788   |                 0 | huggyllama/llama-7b |
| world_knowledge          | triviaqa                         |                                     | 0.443914   |                 0 | huggyllama/llama-7b |
| world_knowledge          | bigbench_qa_wikidata             |                                     | 0.407263   |                 0 | huggyllama/llama-7b |
| world_knowledge          | arc_easy                         |                                     | 0.664983   |                 0 | huggyllama/llama-7b |
| world_knowledge          | arc_challenge                    |                                     | 0.422355   |                 0 | huggyllama/llama-7b |
| world_knowledge          | mmlu                             | Average                             | 0.29916    |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | abstract_algebra                    | 0.32       |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | anatomy                             | 0.340741   |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | astronomy                           | 0.342105   |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | business_ethics                     | 0.35       |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | clinical_knowledge                  | 0.335849   |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | college_biology                     | 0.284722   |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | college_chemistry                   | 0.22       |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | college_computer_science            | 0.26       |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | college_mathematics                 | 0.31       |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | college_medicine                    | 0.300578   |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | college_physics                     | 0.176471   |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | computer_security                   | 0.46       |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | conceptual_physics                  | 0.302128   |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | econometrics                        | 0.263158   |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | electrical_engineering              | 0.22069    |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | elementary_mathematics              | 0.259259   |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | formal_logic                        | 0.246032   |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | global_facts                        | 0.35       |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | high_school_biology                 | 0.325806   |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | high_school_chemistry               | 0.275862   |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | high_school_computer_science        | 0.31       |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | high_school_european_history        | 0.284848   |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | high_school_geography               | 0.30303    |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | high_school_government_and_politics | 0.274611   |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | high_school_macroeconomics          | 0.264103   |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | high_school_mathematics             | 0.207407   |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | high_school_microeconomics          | 0.214286   |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | high_school_physics                 | 0.298013   |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | high_school_psychology              | 0.311927   |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | high_school_statistics              | 0.263889   |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | high_school_us_history              | 0.279412   |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | high_school_world_history           | 0.28692    |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | human_aging                         | 0.273543   |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | human_sexuality                     | 0.335878   |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | international_law                   | 0.380165   |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | jurisprudence                       | 0.351852   |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | logical_fallacies                   | 0.331288   |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | machine_learning                    | 0.196429   |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | management                          | 0.320388   |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | marketing                           | 0.346154   |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | medical_genetics                    | 0.38       |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | miscellaneous                       | 0.389527   |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | moral_disputes                      | 0.315029   |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | moral_scenarios                     | 0.235754   |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | nutrition                           | 0.320261   |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | philosophy                          | 0.366559   |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | prehistory                          | 0.376543   |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | professional_accounting             | 0.262411   |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | professional_law                    | 0.277705   |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | professional_medicine               | 0.172794   |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | professional_psychology             | 0.29085    |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | public_relations                    | 0.263636   |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | security_studies                    | 0.24898    |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | sociology                           | 0.298507   |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | us_foreign_policy                   | 0.4        |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | virology                            | 0.319277   |                 0 | huggyllama/llama-7b |
| world_knowledge          |                                  | world_religions                     | 0.356725   |                 0 | huggyllama/llama-7b |
| world_knowledge          | bigbench_misconceptions          |                                     | 0.474886   |                 0 | huggyllama/llama-7b |
| world_knowledge          | bigbench_movie_recommendation    |                                     | 0.246      |                 0 | huggyllama/llama-7b |
| commonsense_reasoning    | copa                             |                                     | 0.78       |                 0 | huggyllama/llama-7b |
| commonsense_reasoning    | piqa                             |                                     | 0.785092   |                 0 | huggyllama/llama-7b |
| commonsense_reasoning    | openbook_qa                      |                                     | 0.418      |                 0 | huggyllama/llama-7b |
| commonsense_reasoning    | bigbench_novel_concepts          |                                     | 0.46875    |                 0 | huggyllama/llama-7b |
| commonsense_reasoning    | bigbench_strange_stories         |                                     | 0.649425   |                 0 | huggyllama/llama-7b |
| commonsense_reasoning    | bigbench_strategy_qa             |                                     | 0.573176   |                 0 | huggyllama/llama-7b |
| language_understanding   | lambada_openai                   |                                     | 0.73724    |                 0 | huggyllama/llama-7b |
| language_understanding   | hellaswag                        |                                     | 0.751344   |                 0 | huggyllama/llama-7b |
| language_understanding   | winograd                         |                                     | 0.882784   |                 0 | huggyllama/llama-7b |
| language_understanding   | winogrande                       |                                     | 0.700868   |                 0 | huggyllama/llama-7b |
| language_understanding   | bigbench_conlang_translation     |                                     | 0.0487805  |                 0 | huggyllama/llama-7b |
| language_understanding   | bigbench_language_identification |                                     | 0.2699     |                 0 | huggyllama/llama-7b |
| language_understanding   | bigbench_conceptual_combinations |                                     | 0.398058   |                 0 | huggyllama/llama-7b |
| symbolic_problem_solving | bigbench_elementary_math_qa      |                                     | 0.278564   |                 0 | huggyllama/llama-7b |
| symbolic_problem_solving | bigbench_dyck_languages          |                                     | 0          |                 0 | huggyllama/llama-7b |
| symbolic_problem_solving | bigbench_cs_algorithms           |                                     | 0.00984848 |                 0 | huggyllama/llama-7b |
| symbolic_problem_solving | bigbench_logical_deduction       |                                     | 0.288      |                 0 | huggyllama/llama-7b |
| symbolic_problem_solving | bigbench_operators               |                                     | 0.309524   |                 0 | huggyllama/llama-7b |
| symbolic_problem_solving | bigbench_repeat_copy_logic       |                                     | 0          |                 0 | huggyllama/llama-7b |
| symbolic_problem_solving | simple_arithmetic_nospaces       |                                     | 0          |                 0 | huggyllama/llama-7b |
| symbolic_problem_solving | simple_arithmetic_withspaces     |                                     | 0.173      |                 0 | huggyllama/llama-7b |
| symbolic_problem_solving | math_qa                          |                                     | 0.254777   |                 0 | huggyllama/llama-7b |
| symbolic_problem_solving | logi_qa                          |                                     | 0.291859   |                 0 | huggyllama/llama-7b |
| reading_comprehension    | squad                            |                                     | 0.244465   |                 0 | huggyllama/llama-7b |
| reading_comprehension    | coqa                             |                                     | 0.177377   |                 0 | huggyllama/llama-7b |
| reading_comprehension    | bigbench_understanding_fables    |                                     | 0.285714   |                 0 | huggyllama/llama-7b |
| reading_comprehension    | boolq                            |                                     | 0.760856   |                 0 | huggyllama/llama-7b |
| world_knowledge          | jeopardy                         | Average                             | 0.493261   |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | american_history                    | 0.564165   |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | literature                          | 0.644898   |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | science                             | 0.361345   |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | word_origins                        | 0.271233   |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | world_history                       | 0.624665   |                10 | huggyllama/llama-7b |
| world_knowledge          | bigbench_qa_wikidata             |                                     | 0.705034   |                10 | huggyllama/llama-7b |
| world_knowledge          | arc_easy                         |                                     | 0.740741   |                10 | huggyllama/llama-7b |
| world_knowledge          | arc_challenge                    |                                     | 0.46843    |                10 | huggyllama/llama-7b |
| world_knowledge          | mmlu                             | Average                             | 0.328813   |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | abstract_algebra                    | 0.3        |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | anatomy                             | 0.288889   |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | astronomy                           | 0.309211   |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | business_ethics                     | 0.39       |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | clinical_knowledge                  | 0.324528   |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | college_biology                     | 0.319444   |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | college_chemistry                   | 0.21       |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | college_computer_science            | 0.19       |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | college_mathematics                 | 0.32       |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | college_medicine                    | 0.375723   |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | college_physics                     | 0.303922   |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | computer_security                   | 0.39       |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | conceptual_physics                  | 0.319149   |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | econometrics                        | 0.254386   |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | electrical_engineering              | 0.282759   |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | elementary_mathematics              | 0.26455    |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | formal_logic                        | 0.380952   |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | global_facts                        | 0.36       |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | high_school_biology                 | 0.341935   |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | high_school_chemistry               | 0.231527   |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | high_school_computer_science        | 0.37       |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | high_school_european_history        | 0.406061   |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | high_school_geography               | 0.257576   |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | high_school_government_and_politics | 0.362694   |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | high_school_macroeconomics          | 0.305128   |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | high_school_mathematics             | 0.251852   |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | high_school_microeconomics          | 0.264706   |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | high_school_physics                 | 0.238411   |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | high_school_psychology              | 0.352294   |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | high_school_statistics              | 0.342593   |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | high_school_us_history              | 0.377451   |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | high_school_world_history           | 0.362869   |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | human_aging                         | 0.443946   |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | human_sexuality                     | 0.351145   |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | international_law                   | 0.380165   |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | jurisprudence                       | 0.361111   |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | logical_fallacies                   | 0.355828   |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | machine_learning                    | 0.285714   |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | management                          | 0.31068    |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | marketing                           | 0.423077   |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | medical_genetics                    | 0.33       |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | miscellaneous                       | 0.408685   |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | moral_disputes                      | 0.361272   |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | moral_scenarios                     | 0.241341   |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | nutrition                           | 0.362745   |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | philosophy                          | 0.33119    |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | prehistory                          | 0.382716   |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | professional_accounting             | 0.304965   |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | professional_law                    | 0.297262   |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | professional_medicine               | 0.253676   |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | professional_psychology             | 0.313726   |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | public_relations                    | 0.363636   |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | security_studies                    | 0.314286   |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | sociology                           | 0.373134   |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | us_foreign_policy                   | 0.48       |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | virology                            | 0.289157   |                10 | huggyllama/llama-7b |
| world_knowledge          |                                  | world_religions                     | 0.374269   |                10 | huggyllama/llama-7b |
| world_knowledge          | bigbench_misconceptions          |                                     | 0.543379   |                10 | huggyllama/llama-7b |
| world_knowledge          | bigbench_movie_recommendation    |                                     | 0.29       |                10 | huggyllama/llama-7b |
| commonsense_reasoning    | piqa                             |                                     | 0.799238   |                10 | huggyllama/llama-7b |
| commonsense_reasoning    | bigbench_novel_concepts          |                                     | 0.53125    |                10 | huggyllama/llama-7b |
| commonsense_reasoning    | bigbench_strange_stories         |                                     | 0.718391   |                10 | huggyllama/llama-7b |
| commonsense_reasoning    | bigbench_strategy_qa             |                                     | 0.591088   |                10 | huggyllama/llama-7b |
| language_understanding   | hellaswag                        |                                     | 0.765385   |                10 | huggyllama/llama-7b |
| language_understanding   | bigbench_conlang_translation     |                                     | 0.0426829  |                10 | huggyllama/llama-7b |
| language_understanding   | bigbench_language_identification |                                     | 0.2564     |                10 | huggyllama/llama-7b |
| language_understanding   | bigbench_conceptual_combinations |                                     | 0.514563   |                10 | huggyllama/llama-7b |
| symbolic_problem_solving | bigbench_elementary_math_qa      |                                     | 0.261609   |                10 | huggyllama/llama-7b |
| symbolic_problem_solving | bigbench_dyck_languages          |                                     | 0.211      |                10 | huggyllama/llama-7b |
| symbolic_problem_solving | bigbench_cs_algorithms           |                                     | 0.462121   |                10 | huggyllama/llama-7b |
| symbolic_problem_solving | bigbench_logical_deduction       |                                     | 0.268      |                10 | huggyllama/llama-7b |
| symbolic_problem_solving | bigbench_operators               |                                     | 0.442857   |                10 | huggyllama/llama-7b |
| symbolic_problem_solving | bigbench_repeat_copy_logic       |                                     | 0.25       |                10 | huggyllama/llama-7b |
| symbolic_problem_solving | simple_arithmetic_nospaces       |                                     | 0.212      |                10 | huggyllama/llama-7b |
| symbolic_problem_solving | simple_arithmetic_withspaces     |                                     | 0.23       |                10 | huggyllama/llama-7b |
| symbolic_problem_solving | math_qa                          |                                     | 0.256788   |                10 | huggyllama/llama-7b |
| symbolic_problem_solving | logi_qa                          |                                     | 0.222734   |                10 | huggyllama/llama-7b |
| reading_comprehension    | pubmed_qa_labeled                |                                     | 0.569      |                10 | huggyllama/llama-7b |
| reading_comprehension    | squad                            |                                     | 0.563765   |                10 | huggyllama/llama-7b |
| reading_comprehension    | coqa                             |                                     | 0.274082   |                10 | huggyllama/llama-7b |
| reading_comprehension    | bigbench_understanding_fables    |                                     | 0.280423   |                10 | huggyllama/llama-7b |
| reading_comprehension    | boolq                            |                                     | 0.76422    |                10 | huggyllama/llama-7b |
| world_knowledge          | jeopardy                         | Average                             | 0.38609    |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | american_history                    | 0.532688   |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | literature                          | 0.544898   |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | science                             | 0.237395   |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | word_origins                        | 0.0712329  |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | world_history                       | 0.544236   |                 0 | huggyllama/llama-13b   |
| world_knowledge          | triviaqa                         |                                     | 0.512066   |                 0 | huggyllama/llama-13b   |
| world_knowledge          | bigbench_qa_wikidata             |                                     | 0.424585   |                 0 | huggyllama/llama-13b   |
| world_knowledge          | arc_easy                         |                                     | 0.670875   |                 0 | huggyllama/llama-13b   |
| world_knowledge          | arc_challenge                    |                                     | 0.465017   |                 0 | huggyllama/llama-13b   |
| world_knowledge          | mmlu                             | Average                             | 0.311644   |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | abstract_algebra                    | 0.24       |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | anatomy                             | 0.303704   |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | astronomy                           | 0.256579   |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | business_ethics                     | 0.39       |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | clinical_knowledge                  | 0.290566   |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | college_biology                     | 0.361111   |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | college_chemistry                   | 0.23       |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | college_computer_science            | 0.24       |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | college_mathematics                 | 0.29       |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | college_medicine                    | 0.271676   |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | college_physics                     | 0.254902   |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | computer_security                   | 0.39       |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | conceptual_physics                  | 0.314894   |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | econometrics                        | 0.254386   |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | electrical_engineering              | 0.248276   |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | elementary_mathematics              | 0.21164    |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | formal_logic                        | 0.277778   |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | global_facts                        | 0.19       |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | high_school_biology                 | 0.335484   |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | high_school_chemistry               | 0.221675   |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | high_school_computer_science        | 0.29       |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | high_school_european_history        | 0.339394   |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | high_school_geography               | 0.277778   |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | high_school_government_and_politics | 0.321244   |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | high_school_macroeconomics          | 0.241026   |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | high_school_mathematics             | 0.214815   |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | high_school_microeconomics          | 0.260504   |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | high_school_physics                 | 0.192053   |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | high_school_psychology              | 0.352294   |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | high_school_statistics              | 0.157407   |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | high_school_us_history              | 0.372549   |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | high_school_world_history           | 0.388186   |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | human_aging                         | 0.461883   |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | human_sexuality                     | 0.358779   |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | international_law                   | 0.363636   |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | jurisprudence                       | 0.351852   |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | logical_fallacies                   | 0.386503   |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | machine_learning                    | 0.321429   |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | management                          | 0.359223   |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | marketing                           | 0.461538   |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | medical_genetics                    | 0.37       |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | miscellaneous                       | 0.464879   |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | moral_disputes                      | 0.289017   |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | moral_scenarios                     | 0.237989   |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | nutrition                           | 0.359477   |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | philosophy                          | 0.257235   |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | prehistory                          | 0.330247   |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | professional_accounting             | 0.262411   |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | professional_law                    | 0.267275   |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | professional_medicine               | 0.264706   |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | professional_psychology             | 0.377451   |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | public_relations                    | 0.454545   |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | security_studies                    | 0.22449    |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | sociology                           | 0.41791    |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | us_foreign_policy                   | 0.39       |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | virology                            | 0.295181   |                 0 | huggyllama/llama-13b   |
| world_knowledge          |                                  | world_religions                     | 0.45614    |                 0 | huggyllama/llama-13b   |
| world_knowledge          | bigbench_misconceptions          |                                     | 0.511416   |                 0 | huggyllama/llama-13b   |
| world_knowledge          | bigbench_movie_recommendation    |                                     | 0.25       |                 0 | huggyllama/llama-13b   |
| commonsense_reasoning    | copa                             |                                     | 0.83       |                 0 | huggyllama/llama-13b   |
| commonsense_reasoning    | piqa                             |                                     | 0.796518   |                 0 | huggyllama/llama-13b   |
| commonsense_reasoning    | openbook_qa                      |                                     | 0.444      |                 0 | huggyllama/llama-13b   |
| commonsense_reasoning    | bigbench_novel_concepts          |                                     | 0.59375    |                 0 | huggyllama/llama-13b   |
| commonsense_reasoning    | bigbench_strange_stories         |                                     | 0.626437   |                 0 | huggyllama/llama-13b   |
| commonsense_reasoning    | bigbench_strategy_qa             |                                     | 0.598515   |                 0 | huggyllama/llama-13b   |
| language_understanding   | lambada_openai                   |                                     | 0.756453   |                 0 | huggyllama/llama-13b   |
| language_understanding   | hellaswag                        |                                     | 0.784007   |                 0 | huggyllama/llama-13b   |
| language_understanding   | winograd                         |                                     | 0.901099   |                 0 | huggyllama/llama-13b   |
| language_understanding   | winogrande                       |                                     | 0.736385   |                 0 | huggyllama/llama-13b   |
| language_understanding   | bigbench_conlang_translation     |                                     | 0.0731707  |                 0 | huggyllama/llama-13b   |
| language_understanding   | bigbench_language_identification |                                     | 0.3466     |                 0 | huggyllama/llama-13b   |
| language_understanding   | bigbench_conceptual_combinations |                                     | 0.485437   |                 0 | huggyllama/llama-13b   |
| symbolic_problem_solving | bigbench_elementary_math_qa      |                                     | 0.290881   |                 0 | huggyllama/llama-13b   |
| symbolic_problem_solving | bigbench_dyck_languages          |                                     | 0          |                 0 | huggyllama/llama-13b   |
| symbolic_problem_solving | bigbench_cs_algorithms           |                                     | 0.00909091 |                 0 | huggyllama/llama-13b   |
| symbolic_problem_solving | bigbench_logical_deduction       |                                     | 0.251333   |                 0 | huggyllama/llama-13b   |
| symbolic_problem_solving | bigbench_operators               |                                     | 0.352381   |                 0 | huggyllama/llama-13b   |
| symbolic_problem_solving | bigbench_repeat_copy_logic       |                                     | 0.03125    |                 0 | huggyllama/llama-13b   |
| symbolic_problem_solving | simple_arithmetic_nospaces       |                                     | 0          |                 0 | huggyllama/llama-13b   |
| symbolic_problem_solving | simple_arithmetic_withspaces     |                                     | 0.166      |                 0 | huggyllama/llama-13b   |
| symbolic_problem_solving | math_qa                          |                                     | 0.260811   |                 0 | huggyllama/llama-13b   |
| symbolic_problem_solving | logi_qa                          |                                     | 0.268817   |                 0 | huggyllama/llama-13b   |
| reading_comprehension    | squad                            |                                     | 0.240965   |                 0 | huggyllama/llama-13b   |
| reading_comprehension    | coqa                             |                                     | 0.187649   |                 0 | huggyllama/llama-13b   |
| reading_comprehension    | bigbench_understanding_fables    |                                     | 0.449735   |                 0 | huggyllama/llama-13b   |
| reading_comprehension    | boolq                            |                                     | 0.787156   |                 0 | huggyllama/llama-13b   |
| world_knowledge          | jeopardy                         | Average                             | 0.523937   |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | american_history                    | 0.598063   |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | literature                          | 0.718367   |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | science                             | 0.380252   |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | word_origins                        | 0.284932   |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | world_history                       | 0.63807    |                10 | huggyllama/llama-13b   |
| world_knowledge          | bigbench_qa_wikidata             |                                     | 0.740958   |                10 | huggyllama/llama-13b   |
| world_knowledge          | arc_easy                         |                                     | 0.77904    |                10 | huggyllama/llama-13b   |
| world_knowledge          | arc_challenge                    |                                     | 0.53413    |                10 | huggyllama/llama-13b   |
| world_knowledge          | mmlu                             | Average                             | 0.431254   |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | abstract_algebra                    | 0.35       |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | anatomy                             | 0.422222   |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | astronomy                           | 0.414474   |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | business_ethics                     | 0.44       |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | clinical_knowledge                  | 0.437736   |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | college_biology                     | 0.465278   |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | college_chemistry                   | 0.22       |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | college_computer_science            | 0.26       |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | college_mathematics                 | 0.29       |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | college_medicine                    | 0.427746   |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | college_physics                     | 0.303922   |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | computer_security                   | 0.53       |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | conceptual_physics                  | 0.348936   |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | econometrics                        | 0.236842   |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | electrical_engineering              | 0.317241   |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | elementary_mathematics              | 0.269841   |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | formal_logic                        | 0.365079   |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | global_facts                        | 0.31       |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | high_school_biology                 | 0.503226   |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | high_school_chemistry               | 0.29064    |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | high_school_computer_science        | 0.46       |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | high_school_european_history        | 0.545455   |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | high_school_geography               | 0.479798   |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | high_school_government_and_politics | 0.61658    |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | high_school_macroeconomics          | 0.361538   |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | high_school_mathematics             | 0.211111   |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | high_school_microeconomics          | 0.453782   |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | high_school_physics                 | 0.238411   |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | high_school_psychology              | 0.53578    |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | high_school_statistics              | 0.226852   |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | high_school_us_history              | 0.514706   |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | high_school_world_history           | 0.527426   |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | human_aging                         | 0.520179   |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | human_sexuality                     | 0.519084   |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | international_law                   | 0.603306   |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | jurisprudence                       | 0.546296   |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | logical_fallacies                   | 0.404908   |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | machine_learning                    | 0.392857   |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | management                          | 0.514563   |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | marketing                           | 0.662393   |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | medical_genetics                    | 0.46       |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | miscellaneous                       | 0.604087   |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | moral_disputes                      | 0.485549   |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | moral_scenarios                     | 0.263687   |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | nutrition                           | 0.51634    |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | philosophy                          | 0.536977   |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | prehistory                          | 0.5        |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | professional_accounting             | 0.35461    |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | professional_law                    | 0.365711   |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | professional_medicine               | 0.334559   |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | professional_psychology             | 0.457516   |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | public_relations                    | 0.490909   |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | security_studies                    | 0.416327   |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | sociology                           | 0.552239   |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | us_foreign_policy                   | 0.64       |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | virology                            | 0.415663   |                10 | huggyllama/llama-13b   |
| world_knowledge          |                                  | world_religions                     | 0.649123   |                10 | huggyllama/llama-13b   |
| world_knowledge          | bigbench_misconceptions          |                                     | 0.630137   |                10 | huggyllama/llama-13b   |
| world_knowledge          | bigbench_movie_recommendation    |                                     | 0.794      |                10 | huggyllama/llama-13b   |
| commonsense_reasoning    | piqa                             |                                     | 0.805223   |                10 | huggyllama/llama-13b   |
| commonsense_reasoning    | bigbench_novel_concepts          |                                     | 0.6875     |                10 | huggyllama/llama-13b   |
| commonsense_reasoning    | bigbench_strange_stories         |                                     | 0.724138   |                10 | huggyllama/llama-13b   |
| commonsense_reasoning    | bigbench_strategy_qa             |                                     | 0.612495   |                10 | huggyllama/llama-13b   |
| language_understanding   | hellaswag                        |                                     | 0.794463   |                10 | huggyllama/llama-13b   |
| language_understanding   | bigbench_conlang_translation     |                                     | 0.0670732  |                10 | huggyllama/llama-13b   |
| language_understanding   | bigbench_language_identification |                                     | 0.337      |                10 | huggyllama/llama-13b   |
| language_understanding   | bigbench_conceptual_combinations |                                     | 0.572816   |                10 | huggyllama/llama-13b   |
| symbolic_problem_solving | bigbench_elementary_math_qa      |                                     | 0.286897   |                10 | huggyllama/llama-13b   |
| symbolic_problem_solving | bigbench_dyck_languages          |                                     | 0.394      |                10 | huggyllama/llama-13b   |
| symbolic_problem_solving | bigbench_cs_algorithms           |                                     | 0.474242   |                10 | huggyllama/llama-13b   |
| symbolic_problem_solving | bigbench_logical_deduction       |                                     | 0.294667   |                10 | huggyllama/llama-13b   |
| symbolic_problem_solving | bigbench_operators               |                                     | 0.452381   |                10 | huggyllama/llama-13b   |
| symbolic_problem_solving | bigbench_repeat_copy_logic       |                                     | 0.28125    |                10 | huggyllama/llama-13b   |
| symbolic_problem_solving | simple_arithmetic_nospaces       |                                     | 0.275      |                10 | huggyllama/llama-13b   |
| symbolic_problem_solving | simple_arithmetic_withspaces     |                                     | 0.277      |                10 | huggyllama/llama-13b   |
| symbolic_problem_solving | math_qa                          |                                     | 0.274891   |                10 | huggyllama/llama-13b   |
| symbolic_problem_solving | logi_qa                          |                                     | 0.316436   |                10 | huggyllama/llama-13b   |
| reading_comprehension    | pubmed_qa_labeled                |                                     | 0.599      |                10 | huggyllama/llama-13b   |
| reading_comprehension    | squad                            |                                     | 0.536613   |                10 | huggyllama/llama-13b   |
| reading_comprehension    | coqa                             |                                     | 0.283853   |                10 | huggyllama/llama-13b   |
| reading_comprehension    | bigbench_understanding_fables    |                                     | 0.428571   |                10 | huggyllama/llama-13b   |
| reading_comprehension    | boolq                            |                                     | 0.783486   |                10 | huggyllama/llama-13b   |
| world_knowledge          | jeopardy                         | Average                             | 0.0404594  |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | american_history                    | 0.0266344  |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | literature                          | 0.0489796  |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | science                             | 0.0273109  |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | word_origins                        | 0.00821918 |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | world_history                       | 0.0911528  |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          | triviaqa                         |                                     | 0.231769   |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          | bigbench_qa_wikidata             |                                     | 0.327395   |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          | arc_easy                         |                                     | 0.604798   |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          | arc_challenge                    |                                     | 0.348123   |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          | mmlu                             | Average                             | 0.25116    |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | abstract_algebra                    | 0.26       |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | anatomy                             | 0.325926   |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | astronomy                           | 0.302632   |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | business_ethics                     | 0.26       |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | clinical_knowledge                  | 0.222642   |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | college_biology                     | 0.263889   |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | college_chemistry                   | 0.22       |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | college_computer_science            | 0.28       |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | college_mathematics                 | 0.29       |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | college_medicine                    | 0.225434   |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | college_physics                     | 0.215686   |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | computer_security                   | 0.34       |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | conceptual_physics                  | 0.255319   |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | econometrics                        | 0.236842   |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | electrical_engineering              | 0.248276   |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | elementary_mathematics              | 0.232804   |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | formal_logic                        | 0.214286   |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | global_facts                        | 0.25       |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | high_school_biology                 | 0.254839   |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | high_school_chemistry               | 0.300493   |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | high_school_computer_science        | 0.3        |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | high_school_european_history        | 0.242424   |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | high_school_geography               | 0.247475   |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | high_school_government_and_politics | 0.238342   |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | high_school_macroeconomics          | 0.223077   |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | high_school_mathematics             | 0.251852   |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | high_school_microeconomics          | 0.235294   |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | high_school_physics                 | 0.271523   |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | high_school_psychology              | 0.229358   |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | high_school_statistics              | 0.226852   |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | high_school_us_history              | 0.240196   |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | high_school_world_history           | 0.232068   |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | human_aging                         | 0.179372   |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | human_sexuality                     | 0.251908   |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | international_law                   | 0.22314    |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | jurisprudence                       | 0.231481   |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | logical_fallacies                   | 0.312883   |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | machine_learning                    | 0.276786   |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | management                          | 0.203883   |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | marketing                           | 0.286325   |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | medical_genetics                    | 0.22       |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | miscellaneous                       | 0.272031   |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | moral_disputes                      | 0.286127   |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | moral_scenarios                     | 0.243575   |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | nutrition                           | 0.264706   |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | philosophy                          | 0.266881   |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | prehistory                          | 0.280864   |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | professional_accounting             | 0.262411   |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | professional_law                    | 0.250326   |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | professional_medicine               | 0.224265   |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | professional_psychology             | 0.220588   |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | public_relations                    | 0.209091   |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | security_studies                    | 0.220408   |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | sociology                           | 0.258706   |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | us_foreign_policy                   | 0.27       |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | virology                            | 0.228916   |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | world_religions                     | 0.233918   |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          | bigbench_misconceptions          |                                     | 0.479452   |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          | bigbench_movie_recommendation    |                                     | 0.246      |                 0 | EleutherAI/pythia-12b  |
| commonsense_reasoning    | copa                             |                                     | 0.77       |                 0 | EleutherAI/pythia-12b  |
| commonsense_reasoning    | piqa                             |                                     | 0.771491   |                 0 | EleutherAI/pythia-12b  |
| commonsense_reasoning    | openbook_qa                      |                                     | 0.394      |                 0 | EleutherAI/pythia-12b  |
| commonsense_reasoning    | bigbench_novel_concepts          |                                     | 0.40625    |                 0 | EleutherAI/pythia-12b  |
| commonsense_reasoning    | bigbench_strange_stories         |                                     | 0.557471   |                 0 | EleutherAI/pythia-12b  |
| commonsense_reasoning    | bigbench_strategy_qa             |                                     | 0.525557   |                 0 | EleutherAI/pythia-12b  |
| language_understanding   | lambada_openai                   |                                     | 0.701533   |                 0 | EleutherAI/pythia-12b  |
| language_understanding   | hellaswag                        |                                     | 0.673372   |                 0 | EleutherAI/pythia-12b  |
| language_understanding   | winograd                         |                                     | 0.85348    |                 0 | EleutherAI/pythia-12b  |
| language_understanding   | winogrande                       |                                     | 0.636148   |                 0 | EleutherAI/pythia-12b  |
| language_understanding   | bigbench_conlang_translation     |                                     | 0.0426829  |                 0 | EleutherAI/pythia-12b  |
| language_understanding   | bigbench_language_identification |                                     | 0.2568     |                 0 | EleutherAI/pythia-12b  |
| language_understanding   | bigbench_conceptual_combinations |                                     | 0.320388   |                 0 | EleutherAI/pythia-12b  |
| symbolic_problem_solving | bigbench_elementary_math_qa      |                                     | 0.240749   |                 0 | EleutherAI/pythia-12b  |
| symbolic_problem_solving | bigbench_dyck_languages          |                                     | 0.032      |                 0 | EleutherAI/pythia-12b  |
| symbolic_problem_solving | bigbench_cs_algorithms           |                                     | 0.0590909  |                 0 | EleutherAI/pythia-12b  |
| symbolic_problem_solving | bigbench_logical_deduction       |                                     | 0.254      |                 0 | EleutherAI/pythia-12b  |
| symbolic_problem_solving | bigbench_operators               |                                     | 0.161905   |                 0 | EleutherAI/pythia-12b  |
| symbolic_problem_solving | bigbench_repeat_copy_logic       |                                     | 0          |                 0 | EleutherAI/pythia-12b  |
| symbolic_problem_solving | simple_arithmetic_nospaces       |                                     | 0          |                 0 | EleutherAI/pythia-12b  |
| symbolic_problem_solving | simple_arithmetic_withspaces     |                                     | 0.014      |                 0 | EleutherAI/pythia-12b  |
| symbolic_problem_solving | math_qa                          |                                     | 0.248072   |                 0 | EleutherAI/pythia-12b  |
| symbolic_problem_solving | logi_qa                          |                                     | 0.273425   |                 0 | EleutherAI/pythia-12b  |
| reading_comprehension    | squad                            |                                     | 0.131883   |                 0 | EleutherAI/pythia-12b  |
| reading_comprehension    | coqa                             |                                     | 0.153326   |                 0 | EleutherAI/pythia-12b  |
| reading_comprehension    | bigbench_understanding_fables    |                                     | 0.26455    |                 0 | EleutherAI/pythia-12b  |
| reading_comprehension    | boolq                            |                                     | 0.675841   |                 0 | EleutherAI/pythia-12b  |
| world_knowledge          | jeopardy                         | Average                             | 0.338625   |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | american_history                    | 0.38983    |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | literature                          | 0.338776   |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | science                             | 0.252101   |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | word_origins                        | 0.216438   |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | world_history                       | 0.495979   |                10 | EleutherAI/pythia-12b  |
| world_knowledge          | bigbench_qa_wikidata             |                                     | 0.695487   |                10 | EleutherAI/pythia-12b  |
| world_knowledge          | arc_easy                         |                                     | 0.691919   |                10 | EleutherAI/pythia-12b  |
| world_knowledge          | arc_challenge                    |                                     | 0.40273    |                10 | EleutherAI/pythia-12b  |
| world_knowledge          | mmlu                             | Average                             | 0.25799    |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | abstract_algebra                    | 0.23       |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | anatomy                             | 0.237037   |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | astronomy                           | 0.322368   |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | business_ethics                     | 0.35       |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | clinical_knowledge                  | 0.218868   |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | college_biology                     | 0.25       |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | college_chemistry                   | 0.17       |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | college_computer_science            | 0.29       |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | college_mathematics                 | 0.28       |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | college_medicine                    | 0.242775   |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | college_physics                     | 0.27451    |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | computer_security                   | 0.28       |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | conceptual_physics                  | 0.229787   |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | econometrics                        | 0.192982   |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | electrical_engineering              | 0.213793   |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | elementary_mathematics              | 0.251323   |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | formal_logic                        | 0.277778   |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | global_facts                        | 0.27       |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | high_school_biology                 | 0.264516   |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | high_school_chemistry               | 0.216749   |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | high_school_computer_science        | 0.28       |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | high_school_european_history        | 0.218182   |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | high_school_geography               | 0.257576   |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | high_school_government_and_politics | 0.238342   |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | high_school_macroeconomics          | 0.238462   |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | high_school_mathematics             | 0.244444   |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | high_school_microeconomics          | 0.268908   |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | high_school_physics                 | 0.278146   |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | high_school_psychology              | 0.266055   |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | high_school_statistics              | 0.222222   |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | high_school_us_history              | 0.230392   |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | high_school_world_history           | 0.2827     |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | human_aging                         | 0.336323   |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | human_sexuality                     | 0.229008   |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | international_law                   | 0.272727   |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | jurisprudence                       | 0.240741   |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | logical_fallacies                   | 0.214724   |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | machine_learning                    | 0.232143   |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | management                          | 0.174757   |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | marketing                           | 0.320513   |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | medical_genetics                    | 0.32       |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | miscellaneous                       | 0.257982   |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | moral_disputes                      | 0.277457   |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | moral_scenarios                     | 0.246927   |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | nutrition                           | 0.254902   |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | philosophy                          | 0.311897   |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | prehistory                          | 0.308642   |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | professional_accounting             | 0.234043   |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | professional_law                    | 0.242503   |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | professional_medicine               | 0.172794   |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | professional_psychology             | 0.263072   |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | public_relations                    | 0.272727   |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | security_studies                    | 0.318367   |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | sociology                           | 0.273632   |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | us_foreign_policy                   | 0.29       |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | virology                            | 0.26506    |                10 | EleutherAI/pythia-12b  |
| world_knowledge          |                                  | world_religions                     | 0.28655    |                10 | EleutherAI/pythia-12b  |
| world_knowledge          | bigbench_misconceptions          |                                     | 0.47032    |                10 | EleutherAI/pythia-12b  |
| world_knowledge          | bigbench_movie_recommendation    |                                     | 0.256      |                10 | EleutherAI/pythia-12b  |
| commonsense_reasoning    | piqa                             |                                     | 0.779652   |                10 | EleutherAI/pythia-12b  |
| commonsense_reasoning    | bigbench_novel_concepts          |                                     | 0.5        |                10 | EleutherAI/pythia-12b  |
| commonsense_reasoning    | bigbench_strange_stories         |                                     | 0.649425   |                10 | EleutherAI/pythia-12b  |
| commonsense_reasoning    | bigbench_strategy_qa             |                                     | 0.573176   |                10 | EleutherAI/pythia-12b  |
| language_understanding   | hellaswag                        |                                     | 0.683031   |                10 | EleutherAI/pythia-12b  |
| language_understanding   | bigbench_conlang_translation     |                                     | 0.0243902  |                10 | EleutherAI/pythia-12b  |
| language_understanding   | bigbench_language_identification |                                     | 0.2578     |                10 | EleutherAI/pythia-12b  |
| language_understanding   | bigbench_conceptual_combinations |                                     | 0.262136   |                10 | EleutherAI/pythia-12b  |
| symbolic_problem_solving | bigbench_elementary_math_qa      |                                     | 0.268475   |                10 | EleutherAI/pythia-12b  |
| symbolic_problem_solving | bigbench_dyck_languages          |                                     | 0.381      |                10 | EleutherAI/pythia-12b  |
| symbolic_problem_solving | bigbench_cs_algorithms           |                                     | 0.480303   |                10 | EleutherAI/pythia-12b  |
| symbolic_problem_solving | bigbench_logical_deduction       |                                     | 0.252667   |                10 | EleutherAI/pythia-12b  |
| symbolic_problem_solving | bigbench_operators               |                                     | 0.238095   |                10 | EleutherAI/pythia-12b  |
| symbolic_problem_solving | bigbench_repeat_copy_logic       |                                     | 0.09375    |                10 | EleutherAI/pythia-12b  |
| symbolic_problem_solving | simple_arithmetic_nospaces       |                                     | 0.078      |                10 | EleutherAI/pythia-12b  |
| symbolic_problem_solving | simple_arithmetic_withspaces     |                                     | 0.089      |                10 | EleutherAI/pythia-12b  |
| symbolic_problem_solving | math_qa                          |                                     | 0.250754   |                10 | EleutherAI/pythia-12b  |
| symbolic_problem_solving | logi_qa                          |                                     | 0.228879   |                10 | EleutherAI/pythia-12b  |
| reading_comprehension    | pubmed_qa_labeled                |                                     | 0.56       |                10 | EleutherAI/pythia-12b  |
| reading_comprehension    | squad                            |                                     | 0.476254   |                10 | EleutherAI/pythia-12b  |
| reading_comprehension    | coqa                             |                                     | 0.232745   |                10 | EleutherAI/pythia-12b  |
| reading_comprehension    | bigbench_understanding_fables    |                                     | 0.269841   |                10 | EleutherAI/pythia-12b  |
| reading_comprehension    | boolq                            |                                     | 0.618043   |                10 | EleutherAI/pythia-12b  |
| world_knowledge          | jeopardy                         | Average                             | 0.0141735  |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | american_history                    | 0.00726392 |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | literature                          | 0.0102041  |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | science                             | 0.0105042  |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | word_origins                        | 0          |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | world_history                       | 0.0428954  |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          | triviaqa                         |                                     | 0.197207   |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          | bigbench_qa_wikidata             |                                     | 0.249397   |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          | arc_easy                         |                                     | 0.568603   |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          | arc_challenge                    |                                     | 0.325939   |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          | mmlu                             | Average                             | 0.26043    |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | abstract_algebra                    | 0.31       |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | anatomy                             | 0.274074   |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | astronomy                           | 0.368421   |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | business_ethics                     | 0.21       |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | clinical_knowledge                  | 0.241509   |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | college_biology                     | 0.222222   |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | college_chemistry                   | 0.23       |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | college_computer_science            | 0.23       |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | college_mathematics                 | 0.25       |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | college_medicine                    | 0.236994   |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | college_physics                     | 0.196078   |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | computer_security                   | 0.31       |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | conceptual_physics                  | 0.221277   |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | econometrics                        | 0.236842   |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | electrical_engineering              | 0.241379   |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | elementary_mathematics              | 0.253968   |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | formal_logic                        | 0.222222   |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | global_facts                        | 0.29       |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | high_school_biology                 | 0.225806   |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | high_school_chemistry               | 0.256158   |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | high_school_computer_science        | 0.32       |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | high_school_european_history        | 0.248485   |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | high_school_geography               | 0.272727   |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | high_school_government_and_politics | 0.284974   |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | high_school_macroeconomics          | 0.289744   |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | high_school_mathematics             | 0.266667   |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | high_school_microeconomics          | 0.310924   |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | high_school_physics                 | 0.298013   |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | high_school_psychology              | 0.253211   |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | high_school_statistics              | 0.319444   |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | high_school_us_history              | 0.220588   |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | high_school_world_history           | 0.21097    |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | human_aging                         | 0.331839   |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | human_sexuality                     | 0.267176   |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | international_law                   | 0.289256   |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | jurisprudence                       | 0.222222   |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | logical_fallacies                   | 0.282209   |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | machine_learning                    | 0.241071   |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | management                          | 0.165049   |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | marketing                           | 0.273504   |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | medical_genetics                    | 0.26       |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | miscellaneous                       | 0.269476   |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | moral_disputes                      | 0.289017   |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | moral_scenarios                     | 0.246927   |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | nutrition                           | 0.27451    |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | philosophy                          | 0.244373   |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | prehistory                          | 0.253086   |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | professional_accounting             | 0.230496   |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | professional_law                    | 0.265319   |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | professional_medicine               | 0.378676   |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | professional_psychology             | 0.26634    |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | public_relations                    | 0.245455   |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | security_studies                    | 0.240816   |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | sociology                           | 0.258706   |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | us_foreign_policy                   | 0.18       |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | virology                            | 0.283133   |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | world_religions                     | 0.263158   |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          | bigbench_misconceptions          |                                     | 0.465753   |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          | bigbench_movie_recommendation    |                                     | 0.21       |                 0 | EleutherAI/pythia-6.9b |
| commonsense_reasoning    | copa                             |                                     | 0.76       |                 0 | EleutherAI/pythia-6.9b |
| commonsense_reasoning    | piqa                             |                                     | 0.763874   |                 0 | EleutherAI/pythia-6.9b |
| commonsense_reasoning    | openbook_qa                      |                                     | 0.386      |                 0 | EleutherAI/pythia-6.9b |
| commonsense_reasoning    | bigbench_novel_concepts          |                                     | 0.25       |                 0 | EleutherAI/pythia-6.9b |
| commonsense_reasoning    | bigbench_strange_stories         |                                     | 0.522988   |                 0 | EleutherAI/pythia-6.9b |
| commonsense_reasoning    | bigbench_strategy_qa             |                                     | 0.519004   |                 0 | EleutherAI/pythia-6.9b |
| language_understanding   | lambada_openai                   |                                     | 0.666214   |                 0 | EleutherAI/pythia-6.9b |
| language_understanding   | hellaswag                        |                                     | 0.636128   |                 0 | EleutherAI/pythia-6.9b |
| language_understanding   | winograd                         |                                     | 0.791209   |                 0 | EleutherAI/pythia-6.9b |
| language_understanding   | winogrande                       |                                     | 0.610103   |                 0 | EleutherAI/pythia-6.9b |
| language_understanding   | bigbench_conlang_translation     |                                     | 0.0365854  |                 0 | EleutherAI/pythia-6.9b |
| language_understanding   | bigbench_language_identification |                                     | 0.2477     |                 0 | EleutherAI/pythia-6.9b |
| language_understanding   | bigbench_conceptual_combinations |                                     | 0.300971   |                 0 | EleutherAI/pythia-6.9b |
| symbolic_problem_solving | bigbench_elementary_math_qa      |                                     | 0.259224   |                 0 | EleutherAI/pythia-6.9b |
| symbolic_problem_solving | bigbench_dyck_languages          |                                     | 0.005      |                 0 | EleutherAI/pythia-6.9b |
| symbolic_problem_solving | bigbench_cs_algorithms           |                                     | 0.0439394  |                 0 | EleutherAI/pythia-6.9b |
| symbolic_problem_solving | bigbench_logical_deduction       |                                     | 0.272      |                 0 | EleutherAI/pythia-6.9b |
| symbolic_problem_solving | bigbench_operators               |                                     | 0.171429   |                 0 | EleutherAI/pythia-6.9b |
| symbolic_problem_solving | bigbench_repeat_copy_logic       |                                     | 0          |                 0 | EleutherAI/pythia-6.9b |
| symbolic_problem_solving | simple_arithmetic_nospaces       |                                     | 0          |                 0 | EleutherAI/pythia-6.9b |
| symbolic_problem_solving | simple_arithmetic_withspaces     |                                     | 0.01       |                 0 | EleutherAI/pythia-6.9b |
| symbolic_problem_solving | math_qa                          |                                     | 0.248743   |                 0 | EleutherAI/pythia-6.9b |
| symbolic_problem_solving | logi_qa                          |                                     | 0.233487   |                 0 | EleutherAI/pythia-6.9b |
| reading_comprehension    | squad                            |                                     | 0.126017   |                 0 | EleutherAI/pythia-6.9b |
| reading_comprehension    | coqa                             |                                     | 0.135413   |                 0 | EleutherAI/pythia-6.9b |
| reading_comprehension    | bigbench_understanding_fables    |                                     | 0.222222   |                 0 | EleutherAI/pythia-6.9b |
| reading_comprehension    | boolq                            |                                     | 0.629664   |                 0 | EleutherAI/pythia-6.9b |
| world_knowledge          | jeopardy                         | Average                             | 0.283068   |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | american_history                    | 0.326877   |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | literature                          | 0.3        |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | science                             | 0.22479    |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | word_origins                        | 0.156164   |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | world_history                       | 0.407507   |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          | bigbench_qa_wikidata             |                                     | 0.681758   |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          | arc_easy                         |                                     | 0.670455   |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          | arc_challenge                    |                                     | 0.380546   |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          | mmlu                             | Average                             | 0.253104   |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | abstract_algebra                    | 0.33       |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | anatomy                             | 0.266667   |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | astronomy                           | 0.296053   |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | business_ethics                     | 0.28       |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | clinical_knowledge                  | 0.260377   |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | college_biology                     | 0.284722   |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | college_chemistry                   | 0.25       |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | college_computer_science            | 0.19       |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | college_mathematics                 | 0.26       |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | college_medicine                    | 0.190751   |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | college_physics                     | 0.27451    |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | computer_security                   | 0.27       |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | conceptual_physics                  | 0.276596   |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | econometrics                        | 0.22807    |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | electrical_engineering              | 0.241379   |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | elementary_mathematics              | 0.256614   |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | formal_logic                        | 0.166667   |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | global_facts                        | 0.37       |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | high_school_biology                 | 0.245161   |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | high_school_chemistry               | 0.241379   |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | high_school_computer_science        | 0.23       |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | high_school_european_history        | 0.260606   |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | high_school_geography               | 0.222222   |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | high_school_government_and_politics | 0.253886   |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | high_school_macroeconomics          | 0.235897   |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | high_school_mathematics             | 0.222222   |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | high_school_microeconomics          | 0.189076   |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | high_school_physics                 | 0.291391   |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | high_school_psychology              | 0.225688   |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | high_school_statistics              | 0.222222   |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | high_school_us_history              | 0.20098    |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | high_school_world_history           | 0.274262   |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | human_aging                         | 0.29148    |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | human_sexuality                     | 0.244275   |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | international_law                   | 0.31405    |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | jurisprudence                       | 0.324074   |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | logical_fallacies                   | 0.233129   |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | machine_learning                    | 0.276786   |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | management                          | 0.252427   |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | marketing                           | 0.247863   |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | medical_genetics                    | 0.25       |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | miscellaneous                       | 0.273308   |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | moral_disputes                      | 0.268786   |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | moral_scenarios                     | 0.252514   |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | nutrition                           | 0.267974   |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | philosophy                          | 0.263666   |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | prehistory                          | 0.253086   |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | professional_accounting             | 0.258865   |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | professional_law                    | 0.252282   |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | professional_medicine               | 0.165441   |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | professional_psychology             | 0.284314   |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | public_relations                    | 0.290909   |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | security_studies                    | 0.191837   |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | sociology                           | 0.238806   |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | us_foreign_policy                   | 0.23       |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | virology                            | 0.283133   |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          |                                  | world_religions                     | 0.210526   |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          | bigbench_misconceptions          |                                     | 0.420091   |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          | bigbench_movie_recommendation    |                                     | 0.256      |                10 | EleutherAI/pythia-6.9b |
| commonsense_reasoning    | piqa                             |                                     | 0.773123   |                10 | EleutherAI/pythia-6.9b |
| commonsense_reasoning    | bigbench_novel_concepts          |                                     | 0.5        |                10 | EleutherAI/pythia-6.9b |
| commonsense_reasoning    | bigbench_strange_stories         |                                     | 0.603448   |                10 | EleutherAI/pythia-6.9b |
| commonsense_reasoning    | bigbench_strategy_qa             |                                     | 0.532547   |                10 | EleutherAI/pythia-6.9b |
| language_understanding   | hellaswag                        |                                     | 0.644991   |                10 | EleutherAI/pythia-6.9b |
| language_understanding   | bigbench_conlang_translation     |                                     | 0.0304878  |                10 | EleutherAI/pythia-6.9b |
| language_understanding   | bigbench_language_identification |                                     | 0.2517     |                10 | EleutherAI/pythia-6.9b |
| language_understanding   | bigbench_conceptual_combinations |                                     | 0.320388   |                10 | EleutherAI/pythia-6.9b |
| symbolic_problem_solving | bigbench_elementary_math_qa      |                                     | 0.258229   |                10 | EleutherAI/pythia-6.9b |
| symbolic_problem_solving | bigbench_dyck_languages          |                                     | 0.392      |                10 | EleutherAI/pythia-6.9b |
| symbolic_problem_solving | bigbench_cs_algorithms           |                                     | 0.461364   |                10 | EleutherAI/pythia-6.9b |
| symbolic_problem_solving | bigbench_logical_deduction       |                                     | 0.229333   |                10 | EleutherAI/pythia-6.9b |
| symbolic_problem_solving | bigbench_operators               |                                     | 0.27619    |                10 | EleutherAI/pythia-6.9b |
| symbolic_problem_solving | bigbench_repeat_copy_logic       |                                     | 0.0625     |                10 | EleutherAI/pythia-6.9b |
| symbolic_problem_solving | simple_arithmetic_nospaces       |                                     | 0.04       |                10 | EleutherAI/pythia-6.9b |
| symbolic_problem_solving | simple_arithmetic_withspaces     |                                     | 0.051      |                10 | EleutherAI/pythia-6.9b |
| symbolic_problem_solving | math_qa                          |                                     | 0.241033   |                10 | EleutherAI/pythia-6.9b |
| symbolic_problem_solving | logi_qa                          |                                     | 0.215054   |                10 | EleutherAI/pythia-6.9b |
| reading_comprehension    | pubmed_qa_labeled                |                                     | 0.529      |                10 | EleutherAI/pythia-6.9b |
| reading_comprehension    | squad                            |                                     | 0.458278   |                10 | EleutherAI/pythia-6.9b |
| reading_comprehension    | coqa                             |                                     | 0.216084   |                10 | EleutherAI/pythia-6.9b |
| reading_comprehension    | bigbench_understanding_fables    |                                     | 0.216931   |                10 | EleutherAI/pythia-6.9b |
| reading_comprehension    | boolq                            |                                     | 0.649847   |                10 | EleutherAI/pythia-6.9b |
| world_knowledge          | jeopardy                         | Average                             |  0.42342   |                 0 | llama-30b |
| world_knowledge          |                                  | american_history                    |  0.559322  |                 0 | llama-30b |
| world_knowledge          |                                  | literature                          |  0.512245  |                 0 | llama-30b |
| world_knowledge          |                                  | science                             |  0.258403  |                 0 | llama-30b |
| world_knowledge          |                                  | word_origins                        |  0.2       |                 0 | llama-30b |
| world_knowledge          |                                  | world_history                       |  0.587131  |                 0 | llama-30b |
| world_knowledge          | triviaqa                         |                                     |  0.64006   |                 0 | llama-30b |
| world_knowledge          | bigbench_qa_wikidata             |                                     |  0.432804  |                 0 | llama-30b |
| world_knowledge          | arc_easy                         |                                     |  0.720118  |                 0 | llama-30b |
| world_knowledge          | arc_challenge                    |                                     |  0.488055  |                 0 | llama-30b |
| world_knowledge          | mmlu                             | Average                             |  0.491787  |                 0 | llama-30b |
| world_knowledge          |                                  | abstract_algebra                    |  0.31      |                 0 | llama-30b |
| world_knowledge          |                                  | anatomy                             |  0.474074  |                 0 | llama-30b |
| world_knowledge          |                                  | astronomy                           |  0.552632  |                 0 | llama-30b |
| world_knowledge          |                                  | business_ethics                     |  0.5       |                 0 | llama-30b |
| world_knowledge          |                                  | clinical_knowledge                  |  0.501887  |                 0 | llama-30b |
| world_knowledge          |                                  | college_biology                     |  0.513889  |                 0 | llama-30b |
| world_knowledge          |                                  | college_chemistry                   |  0.26      |                 0 | llama-30b |
| world_knowledge          |                                  | college_computer_science            |  0.28      |                 0 | llama-30b |
| world_knowledge          |                                  | college_mathematics                 |  0.31      |                 0 | llama-30b |
| world_knowledge          |                                  | college_medicine                    |  0.433526  |                 0 | llama-30b |
| world_knowledge          |                                  | college_physics                     |  0.264706  |                 0 | llama-30b |
| world_knowledge          |                                  | computer_security                   |  0.61      |                 0 | llama-30b |
| world_knowledge          |                                  | conceptual_physics                  |  0.357447  |                 0 | llama-30b |
| world_knowledge          |                                  | econometrics                        |  0.359649  |                 0 | llama-30b |
| world_knowledge          |                                  | electrical_engineering              |  0.365517  |                 0 | llama-30b |
| world_knowledge          |                                  | elementary_mathematics              |  0.309524  |                 0 | llama-30b |
| world_knowledge          |                                  | formal_logic                        |  0.230159  |                 0 | llama-30b |
| world_knowledge          |                                  | global_facts                        |  0.39      |                 0 | llama-30b |
| world_knowledge          |                                  | high_school_biology                 |  0.548387  |                 0 | llama-30b |
| world_knowledge          |                                  | high_school_chemistry               |  0.325123  |                 0 | llama-30b |
| world_knowledge          |                                  | high_school_computer_science        |  0.48      |                 0 | llama-30b |
| world_knowledge          |                                  | high_school_european_history        |  0.654545  |                 0 | llama-30b |
| world_knowledge          |                                  | high_school_geography               |  0.585859  |                 0 | llama-30b |
| world_knowledge          |                                  | high_school_government_and_politics |  0.678756  |                 0 | llama-30b |
| world_knowledge          |                                  | high_school_macroeconomics          |  0.412821  |                 0 | llama-30b |
| world_knowledge          |                                  | high_school_mathematics             |  0.3       |                 0 | llama-30b |
| world_knowledge          |                                  | high_school_microeconomics          |  0.466387  |                 0 | llama-30b |
| world_knowledge          |                                  | high_school_physics                 |  0.225166  |                 0 | llama-30b |
| world_knowledge          |                                  | high_school_psychology              |  0.653211  |                 0 | llama-30b |
| world_knowledge          |                                  | high_school_statistics              |  0.356481  |                 0 | llama-30b |
| world_knowledge          |                                  | high_school_us_history              |  0.676471  |                 0 | llama-30b |
| world_knowledge          |                                  | high_school_world_history           |  0.71308   |                 0 | llama-30b |
| world_knowledge          |                                  | human_aging                         |  0.64574   |                 0 | llama-30b |
| world_knowledge          |                                  | human_sexuality                     |  0.603053  |                 0 | llama-30b |
| world_knowledge          |                                  | international_law                   |  0.669421  |                 0 | llama-30b |
| world_knowledge          |                                  | jurisprudence                       |  0.574074  |                 0 | llama-30b |
| world_knowledge          |                                  | logical_fallacies                   |  0.631902  |                 0 | llama-30b |
| world_knowledge          |                                  | machine_learning                    |  0.392857  |                 0 | llama-30b |
| world_knowledge          |                                  | management                          |  0.543689  |                 0 | llama-30b |
| world_knowledge          |                                  | marketing                           |  0.760684  |                 0 | llama-30b |
| world_knowledge          |                                  | medical_genetics                    |  0.53      |                 0 | llama-30b |
| world_knowledge          |                                  | miscellaneous                       |  0.692209  |                 0 | llama-30b |
| world_knowledge          |                                  | moral_disputes                      |  0.557803  |                 0 | llama-30b |
| world_knowledge          |                                  | moral_scenarios                     |  0.246927  |                 0 | llama-30b |
| world_knowledge          |                                  | nutrition                           |  0.513072  |                 0 | llama-30b |
| world_knowledge          |                                  | philosophy                          |  0.59164   |                 0 | llama-30b |
| world_knowledge          |                                  | prehistory                          |  0.608025  |                 0 | llama-30b |
| world_knowledge          |                                  | professional_accounting             |  0.365248  |                 0 | llama-30b |
| world_knowledge          |                                  | professional_law                    |  0.385919  |                 0 | llama-30b |
| world_knowledge          |                                  | professional_medicine               |  0.352941  |                 0 | llama-30b |
| world_knowledge          |                                  | professional_psychology             |  0.49183   |                 0 | llama-30b |
| world_knowledge          |                                  | public_relations                    |  0.545455  |                 0 | llama-30b |
| world_knowledge          |                                  | security_studies                    |  0.595918  |                 0 | llama-30b |
| world_knowledge          |                                  | sociology                           |  0.60199   |                 0 | llama-30b |
| world_knowledge          |                                  | us_foreign_policy                   |  0.79      |                 0 | llama-30b |
| world_knowledge          |                                  | virology                            |  0.481928  |                 0 | llama-30b |
| world_knowledge          |                                  | world_religions                     |  0.760234  |                 0 | llama-30b |
| world_knowledge          | bigbench_misconceptions          |                                     |  0.497717  |                 0 | llama-30b |
| world_knowledge          | bigbench_movie_recommendation    |                                     |  0.312     |                 0 | llama-30b |
| commonsense_reasoning    | copa                             |                                     |  0.84      |                 0 | llama-30b |
| commonsense_reasoning    | piqa                             |                                     |  0.81284   |                 0 | llama-30b |
| commonsense_reasoning    | openbook_qa                      |                                     |  0.466     |                 0 | llama-30b |
| commonsense_reasoning    | bigbench_novel_concepts          |                                     |  0.65625   |                 0 | llama-30b |
| commonsense_reasoning    | bigbench_strange_stories         |                                     |  0.724138  |                 0 | llama-30b |
| commonsense_reasoning    | bigbench_strategy_qa             |                                     |  0.643512  |                 0 | llama-30b |
| language_understanding   | lambada_openai                   |                                     |  0.775471  |                 0 | llama-30b |
| language_understanding   | hellaswag                        |                                     |  0.814778  |                 0 | llama-30b |
| language_understanding   | winograd                         |                                     |  0.882784  |                 0 | llama-30b |
| language_understanding   | winogrande                       |                                     |  0.757695  |                 0 | llama-30b |
| language_understanding   | bigbench_conlang_translation     |                                     |  0.103659  |                 0 | llama-30b |
| language_understanding   | bigbench_language_identification |                                     |  0.3743    |                 0 | llama-30b |
| language_understanding   | bigbench_conceptual_combinations |                                     |  0.592233  |                 0 | llama-30b |
| symbolic_problem_solving | bigbench_elementary_math_qa      |                                     |  0.364465  |                 0 | llama-30b |
| symbolic_problem_solving | bigbench_dyck_languages          |                                     |  0.012     |                 0 | llama-30b |
| symbolic_problem_solving | bigbench_cs_algorithms           |                                     |  0.0257576 |                 0 | llama-30b |
| symbolic_problem_solving | bigbench_logical_deduction       |                                     |  0.316     |                 0 | llama-30b |
| symbolic_problem_solving | bigbench_operators               |                                     |  0.380952  |                 0 | llama-30b |
| symbolic_problem_solving | bigbench_repeat_copy_logic       |                                     |  0         |                 0 | llama-30b |
| symbolic_problem_solving | simple_arithmetic_nospaces       |                                     |  0         |                 0 | llama-30b |
| symbolic_problem_solving | simple_arithmetic_withspaces     |                                     |  0.279     |                 0 | llama-30b |
| symbolic_problem_solving | math_qa                          |                                     |  0.270198  |                 0 | llama-30b |
| symbolic_problem_solving | logi_qa                          |                                     |  0.360983  |                 0 | llama-30b |
| reading_comprehension    | squad                            |                                     |  0.283444  |                 0 | llama-30b |
| reading_comprehension    | coqa                             |                                     |  0.1998    |                 0 | llama-30b |
| reading_comprehension    | bigbench_understanding_fables    |                                     |  0.68254   |                 0 | llama-30b |
| reading_comprehension    | boolq                            |                                     |  0.842508  |                 0 | llama-30b |
| world_knowledge          | jeopardy                         | Average                             |  0.570411  |                10 | llama-30b |
| world_knowledge          |                                  | american_history                    |  0.627119  |                10 | llama-30b |
| world_knowledge          |                                  | literature                          |  0.708163  |                10 | llama-30b |
| world_knowledge          |                                  | science                             |  0.432773  |                10 | llama-30b |
| world_knowledge          |                                  | word_origins                        |  0.416438  |                10 | llama-30b |
| world_knowledge          |                                  | world_history                       |  0.66756   |                10 | llama-30b |
| world_knowledge          | bigbench_qa_wikidata             |                                     |  0.744993  |                10 | llama-30b |
| world_knowledge          | arc_easy                         |                                     |  0.818603  |                10 | llama-30b |
| world_knowledge          | arc_challenge                    |                                     |  0.581058  |                10 | llama-30b |
| world_knowledge          | mmlu                             | Average                             |  0.574613  |                10 | llama-30b |
| world_knowledge          |                                  | abstract_algebra                    |  0.25      |                10 | llama-30b |
| world_knowledge          |                                  | anatomy                             |  0.511111  |                10 | llama-30b |
| world_knowledge          |                                  | astronomy                           |  0.638158  |                10 | llama-30b |
| world_knowledge          |                                  | business_ethics                     |  0.59      |                10 | llama-30b |
| world_knowledge          |                                  | clinical_knowledge                  |  0.562264  |                10 | llama-30b |
| world_knowledge          |                                  | college_biology                     |  0.625     |                10 | llama-30b |
| world_knowledge          |                                  | college_chemistry                   |  0.42      |                10 | llama-30b |
| world_knowledge          |                                  | college_computer_science            |  0.46      |                10 | llama-30b |
| world_knowledge          |                                  | college_mathematics                 |  0.36      |                10 | llama-30b |
| world_knowledge          |                                  | college_medicine                    |  0.566474  |                10 | llama-30b |
| world_knowledge          |                                  | college_physics                     |  0.343137  |                10 | llama-30b |
| world_knowledge          |                                  | computer_security                   |  0.64      |                10 | llama-30b |
| world_knowledge          |                                  | conceptual_physics                  |  0.476596  |                10 | llama-30b |
| world_knowledge          |                                  | econometrics                        |  0.298246  |                10 | llama-30b |
| world_knowledge          |                                  | electrical_engineering              |  0.510345  |                10 | llama-30b |
| world_knowledge          |                                  | elementary_mathematics              |  0.34127   |                10 | llama-30b |
| world_knowledge          |                                  | formal_logic                        |  0.293651  |                10 | llama-30b |
| world_knowledge          |                                  | global_facts                        |  0.37      |                10 | llama-30b |
| world_knowledge          |                                  | high_school_biology                 |  0.706452  |                10 | llama-30b |
| world_knowledge          |                                  | high_school_chemistry               |  0.413793  |                10 | llama-30b |
| world_knowledge          |                                  | high_school_computer_science        |  0.59      |                10 | llama-30b |
| world_knowledge          |                                  | high_school_european_history        |  0.745455  |                10 | llama-30b |
| world_knowledge          |                                  | high_school_geography               |  0.752525  |                10 | llama-30b |
| world_knowledge          |                                  | high_school_government_and_politics |  0.818653  |                10 | llama-30b |
| world_knowledge          |                                  | high_school_macroeconomics          |  0.541026  |                10 | llama-30b |
| world_knowledge          |                                  | high_school_mathematics             |  0.262963  |                10 | llama-30b |
| world_knowledge          |                                  | high_school_microeconomics          |  0.588235  |                10 | llama-30b |
| world_knowledge          |                                  | high_school_physics                 |  0.317881  |                10 | llama-30b |
| world_knowledge          |                                  | high_school_psychology              |  0.763303  |                10 | llama-30b |
| world_knowledge          |                                  | high_school_statistics              |  0.453704  |                10 | llama-30b |
| world_knowledge          |                                  | high_school_us_history              |  0.730392  |                10 | llama-30b |
| world_knowledge          |                                  | high_school_world_history           |  0.793249  |                10 | llama-30b |
| world_knowledge          |                                  | human_aging                         |  0.64574   |                10 | llama-30b |
| world_knowledge          |                                  | human_sexuality                     |  0.641221  |                10 | llama-30b |
| world_knowledge          |                                  | international_law                   |  0.727273  |                10 | llama-30b |
| world_knowledge          |                                  | jurisprudence                       |  0.740741  |                10 | llama-30b |
| world_knowledge          |                                  | logical_fallacies                   |  0.699386  |                10 | llama-30b |
| world_knowledge          |                                  | machine_learning                    |  0.428571  |                10 | llama-30b |
| world_knowledge          |                                  | management                          |  0.699029  |                10 | llama-30b |
| world_knowledge          |                                  | marketing                           |  0.837607  |                10 | llama-30b |
| world_knowledge          |                                  | medical_genetics                    |  0.65      |                10 | llama-30b |
| world_knowledge          |                                  | miscellaneous                       |  0.772669  |                10 | llama-30b |
| world_knowledge          |                                  | moral_disputes                      |  0.66763   |                10 | llama-30b |
| world_knowledge          |                                  | moral_scenarios                     |  0.280447  |                10 | llama-30b |
| world_knowledge          |                                  | nutrition                           |  0.607843  |                10 | llama-30b |
| world_knowledge          |                                  | philosophy                          |  0.684887  |                10 | llama-30b |
| world_knowledge          |                                  | prehistory                          |  0.657407  |                10 | llama-30b |
| world_knowledge          |                                  | professional_accounting             |  0.475177  |                10 | llama-30b |
| world_knowledge          |                                  | professional_law                    |  0.466754  |                10 | llama-30b |
| world_knowledge          |                                  | professional_medicine               |  0.533088  |                10 | llama-30b |
| world_knowledge          |                                  | professional_psychology             |  0.607843  |                10 | llama-30b |
| world_knowledge          |                                  | public_relations                    |  0.672727  |                10 | llama-30b |
| world_knowledge          |                                  | security_studies                    |  0.612245  |                10 | llama-30b |
| world_knowledge          |                                  | sociology                           |  0.761194  |                10 | llama-30b |
| world_knowledge          |                                  | us_foreign_policy                   |  0.83      |                10 | llama-30b |
| world_knowledge          |                                  | virology                            |  0.53012   |                10 | llama-30b |
| world_knowledge          |                                  | world_religions                     |  0.789474  |                10 | llama-30b |
| world_knowledge          | bigbench_misconceptions          |                                     |  0.712329  |                10 | llama-30b |
| world_knowledge          | bigbench_movie_recommendation    |                                     |  0.664     |                10 | llama-30b |
| commonsense_reasoning    | piqa                             |                                     |  0.824265  |                10 | llama-30b |
| commonsense_reasoning    | bigbench_novel_concepts          |                                     |  0.65625   |                10 | llama-30b |
| commonsense_reasoning    | bigbench_strange_stories         |                                     |  0.804598  |                10 | llama-30b |
| commonsense_reasoning    | bigbench_strategy_qa             |                                     |  0.68021   |                10 | llama-30b |
| language_understanding   | hellaswag                        |                                     |  0.832105  |                10 | llama-30b |
| language_understanding   | bigbench_conlang_translation     |                                     |  0.140244  |                10 | llama-30b |
| language_understanding   | bigbench_language_identification |                                     |  0.4251    |                10 | llama-30b |
| language_understanding   | bigbench_conceptual_combinations |                                     |  0.757282  |                10 | llama-30b |
| symbolic_problem_solving | bigbench_elementary_math_qa      |                                     |  0.372353  |                10 | llama-30b |
| symbolic_problem_solving | bigbench_dyck_languages          |                                     |  0.369     |                10 | llama-30b |
| symbolic_problem_solving | bigbench_cs_algorithms           |                                     |  0.472727  |                10 | llama-30b |
| symbolic_problem_solving | bigbench_logical_deduction       |                                     |  0.376667  |                10 | llama-30b |
| symbolic_problem_solving | bigbench_operators               |                                     |  0.57619   |                10 | llama-30b |
| symbolic_problem_solving | bigbench_repeat_copy_logic       |                                     |  0.46875   |                10 | llama-30b |
| symbolic_problem_solving | simple_arithmetic_nospaces       |                                     |  0.391     |                10 | llama-30b |
| symbolic_problem_solving | simple_arithmetic_withspaces     |                                     |  0.405     |                10 | llama-30b |
| symbolic_problem_solving | math_qa                          |                                     |  0.275226  |                10 | llama-30b |
| symbolic_problem_solving | logi_qa                          |                                     |  0.374808  |                10 | llama-30b |
| reading_comprehension    | pubmed_qa_labeled                |                                     |  0.595     |                10 | llama-30b |
| reading_comprehension    | squad                            |                                     |  0.623084  |                10 | llama-30b |
| reading_comprehension    | coqa                             |                                     |  0.317174  |                10 | llama-30b |
| reading_comprehension    | bigbench_understanding_fables    |                                     |  0.724868  |                10 | llama-30b |
| reading_comprehension    | boolq                            |                                     |  0.859021  |                10 | llama-30b |
| world_knowledge          | jeopardy                         | Average                             | 0.408168    |                 0 | falcon-40b |
| world_knowledge          |                                  | american_history                    | 0.486683    |                 0 | falcon-40b |
| world_knowledge          |                                  | literature                          | 0.563265    |                 0 | falcon-40b |
| world_knowledge          |                                  | science                             | 0.239496    |                 0 | falcon-40b |
| world_knowledge          |                                  | word_origins                        | 0.158904    |                 0 | falcon-40b |
| world_knowledge          |                                  | world_history                       | 0.592493    |                 0 | falcon-40b |
| world_knowledge          | triviaqa                         |                                     | 0.000530363 |                 0 | falcon-40b |
| world_knowledge          | bigbench_qa_wikidata             |                                     | 0.438906    |                 0 | falcon-40b |
| world_knowledge          | arc_easy                         |                                     | 0.752104    |                 0 | falcon-40b |
| world_knowledge          | arc_challenge                    |                                     | 0.507679    |                 0 | falcon-40b |
| world_knowledge          | mmlu                             | Average                             | 0.492958    |                 0 | falcon-40b |
| world_knowledge          |                                  | abstract_algebra                    | 0.29        |                 0 | falcon-40b |
| world_knowledge          |                                  | anatomy                             | 0.474074    |                 0 | falcon-40b |
| world_knowledge          |                                  | astronomy                           | 0.539474    |                 0 | falcon-40b |
| world_knowledge          |                                  | business_ethics                     | 0.5         |                 0 | falcon-40b |
| world_knowledge          |                                  | clinical_knowledge                  | 0.524528    |                 0 | falcon-40b |
| world_knowledge          |                                  | college_biology                     | 0.541667    |                 0 | falcon-40b |
| world_knowledge          |                                  | college_chemistry                   | 0.28        |                 0 | falcon-40b |
| world_knowledge          |                                  | college_computer_science            | 0.43        |                 0 | falcon-40b |
| world_knowledge          |                                  | college_mathematics                 | 0.33        |                 0 | falcon-40b |
| world_knowledge          |                                  | college_medicine                    | 0.468208    |                 0 | falcon-40b |
| world_knowledge          |                                  | college_physics                     | 0.352941    |                 0 | falcon-40b |
| world_knowledge          |                                  | computer_security                   | 0.63        |                 0 | falcon-40b |
| world_knowledge          |                                  | conceptual_physics                  | 0.421277    |                 0 | falcon-40b |
| world_knowledge          |                                  | econometrics                        | 0.298246    |                 0 | falcon-40b |
| world_knowledge          |                                  | electrical_engineering              | 0.475862    |                 0 | falcon-40b |
| world_knowledge          |                                  | elementary_mathematics              | 0.349206    |                 0 | falcon-40b |
| world_knowledge          |                                  | formal_logic                        | 0.277778    |                 0 | falcon-40b |
| world_knowledge          |                                  | global_facts                        | 0.32        |                 0 | falcon-40b |
| world_knowledge          |                                  | high_school_biology                 | 0.583871    |                 0 | falcon-40b |
| world_knowledge          |                                  | high_school_chemistry               | 0.423645    |                 0 | falcon-40b |
| world_knowledge          |                                  | high_school_computer_science        | 0.52        |                 0 | falcon-40b |
| world_knowledge          |                                  | high_school_european_history        | 0.575758    |                 0 | falcon-40b |
| world_knowledge          |                                  | high_school_geography               | 0.540404    |                 0 | falcon-40b |
| world_knowledge          |                                  | high_school_government_and_politics | 0.647668    |                 0 | falcon-40b |
| world_knowledge          |                                  | high_school_macroeconomics          | 0.464103    |                 0 | falcon-40b |
| world_knowledge          |                                  | high_school_mathematics             | 0.222222    |                 0 | falcon-40b |
| world_knowledge          |                                  | high_school_microeconomics          | 0.487395    |                 0 | falcon-40b |
| world_knowledge          |                                  | high_school_physics                 | 0.311258    |                 0 | falcon-40b |
| world_knowledge          |                                  | high_school_psychology              | 0.684404    |                 0 | falcon-40b |
| world_knowledge          |                                  | high_school_statistics              | 0.337963    |                 0 | falcon-40b |
| world_knowledge          |                                  | high_school_us_history              | 0.573529    |                 0 | falcon-40b |
| world_knowledge          |                                  | high_school_world_history           | 0.594937    |                 0 | falcon-40b |
| world_knowledge          |                                  | human_aging                         | 0.596413    |                 0 | falcon-40b |
| world_knowledge          |                                  | human_sexuality                     | 0.580153    |                 0 | falcon-40b |
| world_knowledge          |                                  | international_law                   | 0.644628    |                 0 | falcon-40b |
| world_knowledge          |                                  | jurisprudence                       | 0.555556    |                 0 | falcon-40b |
| world_knowledge          |                                  | logical_fallacies                   | 0.521472    |                 0 | falcon-40b |
| world_knowledge          |                                  | machine_learning                    | 0.321429    |                 0 | falcon-40b |
| world_knowledge          |                                  | management                          | 0.582524    |                 0 | falcon-40b |
| world_knowledge          |                                  | marketing                           | 0.717949    |                 0 | falcon-40b |
| world_knowledge          |                                  | medical_genetics                    | 0.52        |                 0 | falcon-40b |
| world_knowledge          |                                  | miscellaneous                       | 0.690932    |                 0 | falcon-40b |
| world_knowledge          |                                  | moral_disputes                      | 0.531792    |                 0 | falcon-40b |
| world_knowledge          |                                  | moral_scenarios                     | 0.289385    |                 0 | falcon-40b |
| world_knowledge          |                                  | nutrition                           | 0.545752    |                 0 | falcon-40b |
| world_knowledge          |                                  | philosophy                          | 0.540193    |                 0 | falcon-40b |
| world_knowledge          |                                  | prehistory                          | 0.626543    |                 0 | falcon-40b |
| world_knowledge          |                                  | professional_accounting             | 0.414894    |                 0 | falcon-40b |
| world_knowledge          |                                  | professional_law                    | 0.396349    |                 0 | falcon-40b |
| world_knowledge          |                                  | professional_medicine               | 0.433824    |                 0 | falcon-40b |
| world_knowledge          |                                  | professional_psychology             | 0.495098    |                 0 | falcon-40b |
| world_knowledge          |                                  | public_relations                    | 0.518182    |                 0 | falcon-40b |
| world_knowledge          |                                  | security_studies                    | 0.563265    |                 0 | falcon-40b |
| world_knowledge          |                                  | sociology                           | 0.616915    |                 0 | falcon-40b |
| world_knowledge          |                                  | us_foreign_policy                   | 0.72        |                 0 | falcon-40b |
| world_knowledge          |                                  | virology                            | 0.409639    |                 0 | falcon-40b |
| world_knowledge          |                                  | world_religions                     | 0.795322    |                 0 | falcon-40b |
| world_knowledge          | bigbench_misconceptions          |                                     | 0.589041    |                 0 | falcon-40b |
| world_knowledge          | bigbench_movie_recommendation    |                                     | 0.24        |                 0 | falcon-40b |
| commonsense_reasoning    | copa                             |                                     | 0.87        |                 0 | falcon-40b |
| commonsense_reasoning    | piqa                             |                                     | 0.825354    |                 0 | falcon-40b |
| commonsense_reasoning    | openbook_qa                      |                                     | 0.47        |                 0 | falcon-40b |
| commonsense_reasoning    | bigbench_novel_concepts          |                                     | 0.65625     |                 0 | falcon-40b |
| commonsense_reasoning    | bigbench_strange_stories         |                                     | 0.758621    |                 0 | falcon-40b |
| commonsense_reasoning    | bigbench_strategy_qa             |                                     | 0.620795    |                 0 | falcon-40b |
| language_understanding   | lambada_openai                   |                                     | 0.771977    |                 0 | falcon-40b |
| language_understanding   | hellaswag                        |                                     | 0.838976    |                 0 | falcon-40b |
| language_understanding   | winograd                         |                                     | 0.897436    |                 0 | falcon-40b |
| language_understanding   | winogrande                       |                                     | 0.767167    |                 0 | falcon-40b |
| language_understanding   | bigbench_conlang_translation     |                                     | 0.140244    |                 0 | falcon-40b |
| language_understanding   | bigbench_language_identification |                                     | 0.4101      |                 0 | falcon-40b |
| language_understanding   | bigbench_conceptual_combinations |                                     | 0.427184    |                 0 | falcon-40b |
| symbolic_problem_solving | bigbench_elementary_math_qa      |                                     | 0.370021    |                 0 | falcon-40b |
| symbolic_problem_solving | bigbench_dyck_languages          |                                     | 0           |                 0 | falcon-40b |
| symbolic_problem_solving | bigbench_cs_algorithms           |                                     | 0.0272727   |                 0 | falcon-40b |
| symbolic_problem_solving | bigbench_logical_deduction       |                                     | 0.274667    |                 0 | falcon-40b |
| symbolic_problem_solving | bigbench_operators               |                                     | 0.442857    |                 0 | falcon-40b |
| symbolic_problem_solving | bigbench_repeat_copy_logic       |                                     | 0.0625      |                 0 | falcon-40b |
| symbolic_problem_solving | simple_arithmetic_nospaces       |                                     | 0.041       |                 0 | falcon-40b |
| symbolic_problem_solving | simple_arithmetic_withspaces     |                                     | 0.176       |                 0 | falcon-40b |
| symbolic_problem_solving | math_qa                          |                                     | 0.273885    |                 0 | falcon-40b |
| symbolic_problem_solving | logi_qa                          |                                     | 0.302611    |                 0 | falcon-40b |
| reading_comprehension    | squad                            |                                     | 0.273131    |                 0 | falcon-40b |
| reading_comprehension    | coqa                             |                                     | 0.222473    |                 0 | falcon-40b |
| reading_comprehension    | bigbench_understanding_fables    |                                     | 0.656085    |                 0 | falcon-40b |
| reading_comprehension    | boolq                            |                                     | 0.830581    |                 0 | falcon-40b |
| world_knowledge          | jeopardy                         | Average                             | 0.546622    |                10 | falcon-40b |
| world_knowledge          |                                  | american_history                    | 0.590799    |                10 | falcon-40b |
| world_knowledge          |                                  | literature                          | 0.718367    |                10 | falcon-40b |
| world_knowledge          |                                  | science                             | 0.405462    |                10 | falcon-40b |
| world_knowledge          |                                  | word_origins                        | 0.361644    |                10 | falcon-40b |
| world_knowledge          |                                  | world_history                       | 0.656836    |                10 | falcon-40b |
| world_knowledge          | bigbench_qa_wikidata             |                                     | 0.737661    |                10 | falcon-40b |
| world_knowledge          | arc_easy                         |                                     | 0.821549    |                10 | falcon-40b |
| world_knowledge          | arc_challenge                    |                                     | 0.598976    |                10 | falcon-40b |
| world_knowledge          | mmlu                             | Average                             | 0.566524    |                10 | falcon-40b |
| world_knowledge          |                                  | abstract_algebra                    | 0.3         |                10 | falcon-40b |
| world_knowledge          |                                  | anatomy                             | 0.555556    |                10 | falcon-40b |
| world_knowledge          |                                  | astronomy                           | 0.559211    |                10 | falcon-40b |
| world_knowledge          |                                  | business_ethics                     | 0.59        |                10 | falcon-40b |
| world_knowledge          |                                  | clinical_knowledge                  | 0.558491    |                10 | falcon-40b |
| world_knowledge          |                                  | college_biology                     | 0.625       |                10 | falcon-40b |
| world_knowledge          |                                  | college_chemistry                   | 0.36        |                10 | falcon-40b |
| world_knowledge          |                                  | college_computer_science            | 0.52        |                10 | falcon-40b |
| world_knowledge          |                                  | college_mathematics                 | 0.31        |                10 | falcon-40b |
| world_knowledge          |                                  | college_medicine                    | 0.508671    |                10 | falcon-40b |
| world_knowledge          |                                  | college_physics                     | 0.323529    |                10 | falcon-40b |
| world_knowledge          |                                  | computer_security                   | 0.69        |                10 | falcon-40b |
| world_knowledge          |                                  | conceptual_physics                  | 0.425532    |                10 | falcon-40b |
| world_knowledge          |                                  | econometrics                        | 0.350877    |                10 | falcon-40b |
| world_knowledge          |                                  | electrical_engineering              | 0.544828    |                10 | falcon-40b |
| world_knowledge          |                                  | elementary_mathematics              | 0.322751    |                10 | falcon-40b |
| world_knowledge          |                                  | formal_logic                        | 0.34127     |                10 | falcon-40b |
| world_knowledge          |                                  | global_facts                        | 0.32        |                10 | falcon-40b |
| world_knowledge          |                                  | high_school_biology                 | 0.683871    |                10 | falcon-40b |
| world_knowledge          |                                  | high_school_chemistry               | 0.512315    |                10 | falcon-40b |
| world_knowledge          |                                  | high_school_computer_science        | 0.63        |                10 | falcon-40b |
| world_knowledge          |                                  | high_school_european_history        | 0.666667    |                10 | falcon-40b |
| world_knowledge          |                                  | high_school_geography               | 0.691919    |                10 | falcon-40b |
| world_knowledge          |                                  | high_school_government_and_politics | 0.792746    |                10 | falcon-40b |
| world_knowledge          |                                  | high_school_macroeconomics          | 0.530769    |                10 | falcon-40b |
| world_knowledge          |                                  | high_school_mathematics             | 0.277778    |                10 | falcon-40b |
| world_knowledge          |                                  | high_school_microeconomics          | 0.617647    |                10 | falcon-40b |
| world_knowledge          |                                  | high_school_physics                 | 0.311258    |                10 | falcon-40b |
| world_knowledge          |                                  | high_school_psychology              | 0.726605    |                10 | falcon-40b |
| world_knowledge          |                                  | high_school_statistics              | 0.412037    |                10 | falcon-40b |
| world_knowledge          |                                  | high_school_us_history              | 0.75        |                10 | falcon-40b |
| world_knowledge          |                                  | high_school_world_history           | 0.683544    |                10 | falcon-40b |
| world_knowledge          |                                  | human_aging                         | 0.726457    |                10 | falcon-40b |
| world_knowledge          |                                  | human_sexuality                     | 0.717557    |                10 | falcon-40b |
| world_knowledge          |                                  | international_law                   | 0.735537    |                10 | falcon-40b |
| world_knowledge          |                                  | jurisprudence                       | 0.703704    |                10 | falcon-40b |
| world_knowledge          |                                  | logical_fallacies                   | 0.625767    |                10 | falcon-40b |
| world_knowledge          |                                  | machine_learning                    | 0.401786    |                10 | falcon-40b |
| world_knowledge          |                                  | management                          | 0.728155    |                10 | falcon-40b |
| world_knowledge          |                                  | marketing                           | 0.833333    |                10 | falcon-40b |
| world_knowledge          |                                  | medical_genetics                    | 0.64        |                10 | falcon-40b |
| world_knowledge          |                                  | miscellaneous                       | 0.748404    |                10 | falcon-40b |
| world_knowledge          |                                  | moral_disputes                      | 0.67052     |                10 | falcon-40b |
| world_knowledge          |                                  | moral_scenarios                     | 0.272626    |                10 | falcon-40b |
| world_knowledge          |                                  | nutrition                           | 0.627451    |                10 | falcon-40b |
| world_knowledge          |                                  | philosophy                          | 0.63344     |                10 | falcon-40b |
| world_knowledge          |                                  | prehistory                          | 0.654321    |                10 | falcon-40b |
| world_knowledge          |                                  | professional_accounting             | 0.443262    |                10 | falcon-40b |
| world_knowledge          |                                  | professional_law                    | 0.424381    |                10 | falcon-40b |
| world_knowledge          |                                  | professional_medicine               | 0.591912    |                10 | falcon-40b |
| world_knowledge          |                                  | professional_psychology             | 0.544118    |                10 | falcon-40b |
| world_knowledge          |                                  | public_relations                    | 0.609091    |                10 | falcon-40b |
| world_knowledge          |                                  | security_studies                    | 0.64898     |                10 | falcon-40b |
| world_knowledge          |                                  | sociology                           | 0.761194    |                10 | falcon-40b |
| world_knowledge          |                                  | us_foreign_policy                   | 0.84        |                10 | falcon-40b |
| world_knowledge          |                                  | virology                            | 0.421687    |                10 | falcon-40b |
| world_knowledge          |                                  | world_religions                     | 0.795322    |                10 | falcon-40b |
| world_knowledge          | bigbench_misconceptions          |                                     | 0.689498    |                10 | falcon-40b |
| world_knowledge          | bigbench_movie_recommendation    |                                     | 0.54        |                10 | falcon-40b |
| commonsense_reasoning    | piqa                             |                                     | 0.845484    |                10 | falcon-40b |
| commonsense_reasoning    | bigbench_novel_concepts          |                                     | 0.78125     |                10 | falcon-40b |
| commonsense_reasoning    | bigbench_strange_stories         |                                     | 0.775862    |                10 | falcon-40b |
| commonsense_reasoning    | bigbench_strategy_qa             |                                     | 0.65924     |                10 | falcon-40b |
| language_understanding   | hellaswag                        |                                     | 0.847042    |                10 | falcon-40b |
| language_understanding   | bigbench_conlang_translation     |                                     | 0.140244    |                10 | falcon-40b |
| language_understanding   | bigbench_language_identification |                                     | 0.4297      |                10 | falcon-40b |
| language_understanding   | bigbench_conceptual_combinations |                                     | 0.601942    |                10 | falcon-40b |
| symbolic_problem_solving | bigbench_elementary_math_qa      |                                     | 0.383595    |                10 | falcon-40b |
| symbolic_problem_solving | bigbench_dyck_languages          |                                     | 0.478       |                10 | falcon-40b |
| symbolic_problem_solving | bigbench_cs_algorithms           |                                     | 0.461364    |                10 | falcon-40b |
| symbolic_problem_solving | bigbench_logical_deduction       |                                     | 0.284667    |                10 | falcon-40b |
| symbolic_problem_solving | bigbench_operators               |                                     | 0.514286    |                10 | falcon-40b |
| symbolic_problem_solving | bigbench_repeat_copy_logic       |                                     | 0.28125     |                10 | falcon-40b |
| symbolic_problem_solving | simple_arithmetic_nospaces       |                                     | 0.266       |                10 | falcon-40b |
| symbolic_problem_solving | simple_arithmetic_withspaces     |                                     | 0.276       |                10 | falcon-40b |
| symbolic_problem_solving | math_qa                          |                                     | 0.272544    |                10 | falcon-40b |
| symbolic_problem_solving | logi_qa                          |                                     | 0.364055    |                10 | falcon-40b |
| reading_comprehension    | pubmed_qa_labeled                |                                     | 0.572       |                10 | falcon-40b |
| reading_comprehension    | squad                            |                                     | 0.615232    |                10 | falcon-40b |
| reading_comprehension    | coqa                             |                                     | 0.321433    |                10 | falcon-40b |
| reading_comprehension    | bigbench_understanding_fables    |                                     | 0.624339    |                10 | falcon-40b |
| reading_comprehension    | boolq                            |                                     | 0.836697    |                10 | falcon-40b |
